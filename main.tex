\newif\ifpictures
\picturestrue

\newif\ifjournal
\journaltrue

\newif\ifArXiv
\ArXivfalse

\documentclass[12pt]{amsart}
\usepackage{amssymb,amsmath}
 \usepackage{amsopn}
 \usepackage{xspace}
  \usepackage{hyperref}
 \usepackage[dvips]{graphicx}
\usepackage[arrow,matrix,curve]{xy}
\usepackage {color, tikz}
\usepackage{wasysym}


\headheight=8pt
\topmargin=30pt 
\textheight=611pt     \textwidth=456pt
\oddsidemargin=6pt   \evensidemargin=6pt

\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{remark}[thm]{Remark}


\theoremstyle{definition}

\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}

\numberwithin{thm}{section}


\newcounter{FNC}[page]
\def\newfootnote#1{{\addtocounter{FNC}{2}$^\fnsymbol{FNC}$%
     \let\thefootnote\relax\footnotetext{$^\fnsymbol{FNC}$#1}}}

\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\B}{\mathbb{B}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\TP}{\mathbb{TP}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\tri}{\triangle}
\newcommand{\lf}{\left}
\newcommand{\ri}{\right}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow} 
\newcommand{\Ra}{\Rightarrow}
\newcommand{\La}{\Leftarrow}
\newcommand{\Lera}{\Leftrightarrow}
\newcommand{\ovl}{\overline}
\newcommand{\wh}{\widehat}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}

\newcommand\cA{{\ensuremath{\mathcal{A}}}\xspace}
\newcommand\cB{{\ensuremath{\mathcal{B}}}\xspace}
\newcommand\cC{{\ensuremath{\mathcal{C}}}\xspace}
\newcommand\cD{{\ensuremath{\mathcal{D}}}\xspace}
\newcommand\cE{{\ensuremath{\mathcal{E}}}\xspace}
\newcommand\cF{{\ensuremath{\mathcal{F}}}\xspace}
\newcommand\cG{{\ensuremath{\mathcal{G}}}\xspace}
\newcommand\cH{{\ensuremath{\mathcal{H}}}\xspace}
\newcommand\cI{{\ensuremath{\mathcal{I}}}\xspace}
\newcommand\cJ{{\ensuremath{\mathcal{J}}}\xspace}
\newcommand\cK{{\ensuremath{\mathcal{K}}}\xspace}
\newcommand\cL{{\ensuremath{\mathcal{L}}}\xspace}
\newcommand\cM{{\ensuremath{\mathcal{M}}}\xspace}
\newcommand\cN{{\ensuremath{\mathcal{N}}}\xspace}
\newcommand\cO{{\ensuremath{\mathcal{O}}}\xspace}
\newcommand\cP{{\ensuremath{\mathcal{P}}}\xspace}
\newcommand\cQ{{\ensuremath{\mathcal{Q}}}\xspace}
\newcommand\cR{{\ensuremath{\mathcal{R}}}\xspace}
\newcommand\cS{{\ensuremath{\mathcal{S}}}\xspace}
\newcommand\cT{{\ensuremath{\mathcal{T}}}\xspace}
\newcommand\cU{{\ensuremath{\mathcal{U}}}\xspace}
\newcommand\cV{{\ensuremath{\mathcal{V}}}\xspace}
\newcommand\cW{{\ensuremath{\mathcal{W}}}\xspace}
\newcommand\cX{{\ensuremath{\mathcal{X}}}\xspace}
\newcommand\cY{{\ensuremath{\mathcal{Y}}}\xspace}
\newcommand\cZ{{\ensuremath{\mathcal{Z}}}\xspace}

\newcommand{\eps}{\varepsilon}
\newcommand{\vphi}{\varphi}
\newcommand{\alp}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\kro}{\delta}
\newcommand{\sig}{\sigma}
\newcommand{\Sig}{\Sigma}
\newcommand{\lap}{\Delta}
\newcommand{\Gam}{\Gamma}


\definecolor{DarkGreen}{rgb}{0,0.65,0}
\newcommand{\yixuan}[1]{{\color{orange} \sf $\clubsuit\clubsuit\clubsuit$ Yixuan: [#1]}}
\newcommand{\mareike}[1]{{\color{cyan} \sf $\clubsuit\clubsuit\clubsuit$ Mareike: [#1]}}
\newcommand{\assum}{{\color{red} \sf $(\clubsuit)$}}
\newcommand{\durch}[1]{\textcolor{red}{\sout{#1}}}


\newcommand{\struc}[1]{{\color{blue} #1}}
\newcommand{\alert}[1]{{\color{red} #1}}


\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\tconv}{tconv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\trop}{trop}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\New}{New}
\DeclareMathOperator{\tdeg}{tdeg}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\re}{re}
\DeclareMathOperator{\im}{im} 
\DeclareMathOperator{\odd}{odd} 
\DeclareMathOperator{\even}{even} 
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Circ}{Circ}

\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\RE}{Re}
\DeclareMathOperator{\IM}{Im}
\DeclareMathOperator{\w}{\wedge}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\Val}{Val}
\DeclareMathOperator{\coA}{\text{co}\cA}
\DeclareMathOperator{\coL}{\text{co}\cL}
\DeclareMathOperator{\eq}{eq}
\DeclareMathOperator{\app}{app}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ini}{in}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\SOS}{SOS}
\DeclareMathOperator{\prep}{Prep}

\DeclareMathOperator{\verti}{Vert}
\DeclareMathOperator{\coeffs}{coeffs}


\def\endexa{\hfill$\hexagon$}

\title{Honors Project}

\author{Yixuan Zhou} \author{Mareike Dressler} %\author{}

\address{8514 Villa La Jolla Drive \# 114, La Jolla, CA, 92037\medskip}
\email{yiz044@ucsd.edu}

\address{9500 Gilmam Drive \# 0112, La Jolla, CA, 92093 \medskip}
\email{mdressler@ucsd.edu}



%\subjclass[2010]{Primary: 14P10, 90C25; Secondary: 12D05, 52B20}
\keywords{some keywords go here}


\begin{document}
 

\begin{abstract}
Short description
\end{abstract}

\maketitle


\section{Introduction} 
Polynomial with real coefficients is a very powerful tool in terms of expressing problems. 
It has a vast range of applications, in both theory side and engineering side. 
Deciding the nonegativity of a multivariate polynomial, natually, become 
a cenctral prblem for many optimizations and feasibility problems. 
However, it is known that deciding the nonegativity of an arbitrary multivariate 
polynomial is a NP-hard problem when the degree of the polynomial is greater than or
equals to 2. Therefore, we restrict our focus to decide whether the polynomial can be 
expressed in \emph{sum of squares (SOS)} form. 

\begin{definition}
     Sum of Suqares (SOS)

     Let $\mathbb{R}[x]_{n, d}$ denotes the set of polynomials with $n$ real variables with
     degree at most $d$.
     A polynomial $p(x) \in \mathbb{R}[x]_{n, 2d}$, 
     is a \emph{sum of squares} if
     there exists $q_1(x),...,q_n(x) \in \mathbb{R}[x]_{n, d}$ such that
     \begin{equation}
          p(x) = \sum_{i = 1}^{n} q_i ^2 (x)
     \end{equation}
\end{definition}

It is clear that if a polynomial $p(x)$ is a SOS, then it is nonegative,
Thus, SOS is a (proper) subset of the set of nonegative polynomials. 

We are interested in SOS beacuse given a multivariate polynomial, the decision
problem of whether it can be decomposed into SOS is a polynomial time problem, by using
\emph{Semidefinite Programing (SDP)}.


\begin{thm}
     A multivariate polynomial $p(x) \in \mathbb{R}[x]_{n, 2d}$ is a sum of squares
     if and only if there exists $\mathcal{Q} \in \mathcal{S}^{{n+d \choose d}}$ such that
     
     \begin{equation}
          p(x) = [x]_d^T \mathcal{Q} [x]_d \quad \mathcal{Q} \succcurlyeq 0
     \end{equation}

     Where $[x]_d$ denotes the vector of monomials with degree at most $d$.
\end{thm}

\cite{Blekherman:Parrilo:Thomas}

\cite{Laurent:Survey}

As a consequence of the above theorem, whether a multivariate polynomial is SOS can be determined 
by a \emph{Semidefinite Programming}. Notice that, the size of the semidefinite matrix,
${n+d \choose d}$, when fixing $d$, grows in polynomial time with respect to $n$, and when 
fixing $n$, it grows in polynomial time with respect to $d$. Thus, though imposed some 
limitation, we have "redueced" a NP-Problem to a P problem. 

In the above theorem, there is nothing special with monomials other than being a basis of the vector space $\mathbb{R}[x]_{n,2d}$. 
Instead of using $[x]_d$ to proceed the calculation,
we can choose any basis of the vector space of polynomials $\mathbb{R}[x]_{n,d}$. 
For example, we can choose Lagrange basis, Chebychev basis, ... And in the most cases,
the monomial basis is not the right choice due to its instability of monomials bases.

In this paper, we will use python program to analyze the computational effeciency and 
numerical stability of the usage of different bases. In particular, the analysis will
be focusing on the condition number of the linear system generated when solving the 
semidefinite program, that is generated by $p(x) = [x]_d^T \mathcal{Q} [x]_d $. 
We will use the \emph{condition number} of the matrix in the linear system to identify how stable the
it is under noises that are introduced in practical problems.  
We will further attempt to identify some most effective bases for some particular
type of polynomials, and will try to justify the reasons.  


\newpage

\section{Preliminaries}
\label{Sec:Preliminaries}

Here background definitions etc will be put. 
You can do it in several subsections (like notation, bla, blabla)

\subsection{Notations and Definiations}
\label{Sec:Notations and Definiations}

Because we are going to use a lot of tools from lienar algebra, we first introduce some key definiations that we will use in this thesis.

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times n}$, we say it is \emph{symmetric} if $A^T = A$.
\end{definition}


\begin{thm} Spectrum Theorem

     Given a symmetric matrix$A \in \mathbb{R}^{n \times n}$, it can be diagnoalized as \begin{equation} A = P^{-1}DP \end{equation} where $D$ is the diagnoal matrix with all real values, and $P$ is an orthonomal matrix.
     
     In other word, $A$ has all real eigenvalues, and their correspdoning eigenvectors form an orthonomal basis of $\mathbb{R}^n$.
     
\end{thm}


\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times n}$, it is \emph{positive semidifinate} if $A$ is symmetric and \begin{equation}
          x^T A x \geq 0 \quad \forall x \in \mathbb{R}^n
     \end{equation}
     And we denote it as $A \succcurlyeq 0$.
\end{definition}

\begin{prop}
     A matrix is \emph{positive semidifinate} if and only if all its eigenvalues are greater than or equals to 0
\end{prop}

\begin{proof}
     If a matrix $A$ has eigenvalue $\lambda < 0$, then if $x$ is the correspdoning eigenvector, we have $x^T A x = \lambda x^T x < 0$.
     On the other hand, if $A$ has all positive eignevalues, because $A$ being a symmetric matrix, its eigenvalue form a basis. Thus for any $x \in \mathbb{R}^n$, we have
     $x = \sum_{i = 1} ^ n c_i v_i $ where $v_i$ are the eigenvectors of $A$, that are also orthnormal to each other.
     Hence, $x^T A x$ = $\sum_{i = 1} ^ n \lambda_i$. Since all $\lambda_i \geq 0$, we have that $x ^ T A x \geq 0$.

\end{proof}

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times m}$, the \emph{pesudo-inverse},
     which is also knows as the \emph{Moore-Penrose} inverse of $A$, is the matrix
     $A^+$ satisfying:
     \begin{itemize}
          \item $A A^+ A = A$
          \item $A^+ A A^+ = A^+$
          \item $(A A^+)^T = A A^+$
          \item $(A^+ A)^T = A A^+$
        \end{itemize}
     Every matrix has its pesudo-inverse, and when $A \in \mathbb{R}^{n \times m}$ is \emph{full rank}, 
     that is $rank(A) = min\{n, m\}$, $A$ can be expressed in simple algebric form.
     
     In particular, when $A$ has linearly independent columns, $A^+$ can be computed as
     \begin{equation}
          A^+ = (A^T A)^{-1} A^T
     \end{equation}
     In this case, the pesudo-inverse is called the \emph{left inverse} since $A^+ A = I$.

     And when $A$ has linearly independent rows, $A^+$ can be be computed as
     \begin{equation}
          A^+ = A^T (A A^T)^{-1}
     \end{equation}
     In this case, the pesudo-inverse is called the \emph{right inverse} since $A A^+ = I$. 
\end{definition}

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times m}$, the condition number of $A$, $\kappa(A)$ is defined as
     \begin{equation}
          \kappa(A) = ||A|| \cdot ||A^+||
     \end{equation}
     for any norm imposed on $A$, for instance \emph{Frobenius norm}.
\end{definition}

\begin{remark}
     The condition number measure how stable the system is. It can be alternatively defined as
     \begin{equation}
          \kappa(A) = \frac{\sigma_{max} (A)}{\sigma_{min} (A)}
     \end{equation}
     where the $\sigma$ denotes the singular values of $A$.

     Thus, it can be understand as how stale our system is. 
     Intuiatively, when the condition number is large, some error in the input along the max direction of the singular value,
     our result would largly flacuate because the error, maginified by the singular value, will dominate the input that is along
     the direction of the minimum singular value. 
     Therefore, the smaller the condition number is, the more stable our system is under flcuations caused by noises.
     The rigirous explaination of the condition numebr can be found in \cite{Cheney:Kincaid}
\end{remark}

Now with the tools from linear algebra, we are ready to proceed to the realm of real coefficients polynomials.

\begin{definition}
     Let $\mathbb{R}[x]_{n,d}$ denotes the set of real coefficient 
     polynmials with $n$ variables and at most $d$ degree.
\end{definition}

\begin{definition}
     Let $P_{n, 2d}$ denotes the set of nonegative polynomials with 
     $n$ variables and at most $2d$ degree, that is 
     \begin{equation}
          P_{n, 2d} = \{ p \in \mathbb{R}[x]_{n, 2d}: p(x) \geq 0, \forall x \in \mathbb{R}^d \}
     \end{equation}
\end{definition}

\begin{remark}
     When trying to determine the nonnegativity of a polynomial, there is no reason to consider the set $P_{n, d}$ when $d$ is odd, since if the 
     degree of a polynomial is odd, then it will always be nonegative at some point.
\end{remark}

\begin{definition}
     Let $\Sigma_{n,2d}$ denotes the set of polynomials with $n$ variables and at most
     $d$ degree that are \emph{Sum of Suqares}, that is
     \begin{equation}
          \Sigma_{n, 2d} = \{ p \in \mathbb{R}[x]_{n, 2d}: \exists \text{ } q_1(x), ..., q_k(x) \in \mathbb{R}[x]_{n,d} \text{ } s.t. \text{ }  p(x) = \sum_{i=1}^k q_i^2(x)\}
     \end{equation}     
\end{definition}

\begin{remark}
     It is easy to check that $P_{n, 2d}$ form an vector space. There are a lot of choices of basis, and the most canonical one is the monomial basis.
\end{remark}

\begin{example}
     $P_(2, 2)$ is a vector space, then $\mathcal{B}_{2, 2} = {x^2, xy, y^2, x, y, 1}$ is a basis of the vector space.
\end{example}

\begin{remark}
     Given $\mathcal{B}_{n, d}$ be a basis of $P_{n, d}$. If we list the elements of $\mathcal{B}_{n, d}$ in a column vector, write as $b$, then $b b^T$ form a matrix whose upper triangle entries can be collected to form a basis of $P_{n, 2d}$.
\end{remark}

\begin{example}
     Let $\mathcal{B}_{2, 1} = {x, y, 1}$, be a basis of $P_{2, 1}$, then 
     \begin{equation}
          \begin{bmatrix}
               x \\
               y \\
               1
          \end{bmatrix}
          \begin{bmatrix}
               x & y & 1
          \end{bmatrix}
          = \begin{bmatrix}
               x^2 & xy & x \\
               xy & y^2 & y \\
               x & y & 1 \\
          \end{bmatrix}
     \end{equation}
     Then the upper triangle of the result form a basis of $P_{2, 2}$.
\end{example}

Then the question that we are interested in is given any polynomial $p(x) \in P_{n,2d}$, decide whether it is in $\Sigma_{n, 2d}$. 
To help solving this decision problem, the following proposition will become very helpful.

\begin{prop}
     Given $p(x) \in P_{n, 2d}$, if $p(x) \in \Sigma_{n, 2d}$, then for any basis $\mathcal{B}_{n, d}$ of $P_n{n, d}$, there exists a matrix $ \mathcal{Q} \curlyeqsucc 0$ such that
     \begin{equation}
          \mathcal{B}_{n, d} ^ T \mathcal{Q} \mathcal{B}_{n, d} = p(x)
     \end{equation}
\end{prop}

\begin{proof}
     For any $p(x) \in P_{n, 2d}$, if $p(x) \in \Sigma_{n, 2d}$, then we can write 
     \begin{equation}
          p(x) = \sum_{i = 1} ^ k q^2(x) = \begin{bmatrix} q_1(x),... ,q_k(x)] \end{bmatrix}  \begin{bmatrix} q_1 \\ \vdots \\ q_{k} \end{bmatrix}
     \end{equation}
     Notice that $q_j(x) \in P_{n, d}.$ 
     
     Now given $\mathcal{B}_{n, d} = \{b_1, ..., b_{n + d \choose d}\}$ be a basis of $P_{n, d},$ 
     we have 
     \begin{equation} q_j(x) = \sum_{i = 1}^{n + d \choose d} c_j b_j = \begin{bmatrix} c_1,..., c_{n + d \choose d}] \end{bmatrix} \begin{bmatrix} b_1 \\ \vdots \\ b_{n + d \choose d} \end{bmatrix} \end{equation}

     By subsituting the section equation into the first, we have
     \begin{equation}
          p(x) = 
               \begin{bmatrix} b_1 & ...& b_{n + d \choose d}
               \end{bmatrix} 
               \begin{bmatrix}
               c_{1,1} & ... & c_{1,k} \\
               \vdots\\
               c_{{n + d \choose d},1} & ... & c_{{n + d \choose d},k}
               \end{bmatrix}
               \begin{bmatrix}
                    c_{1,1} & ... & c_{1,{n + d \choose d}} \\
                    \vdots\\
                    c_{k,1} & ... & c_{k, {n + d \choose d}}
               \end{bmatrix}
               \begin{bmatrix} b_1 \\ \vdots \\ b_{n + d \choose d}
               \end{bmatrix} 
     \end{equation}

     Now the matrices in the middle is $C^T C = \mathcal{Q}$ a positive semidefinite matrix, which proofs the foward direction of this theorem.

     On the other hand, if we know $p(x) = \mathcal{B}_{n, d}^T \mathcal{Q} \mathcal{B}_{n, d}$, where $\mathcal{Q}$ is a positive semidefinite matrix, we can just apply the Cholesky decomposition to get $\mathcal{Q} = L^T L$, and recover the SOS form of $p(x)$. \cite{Blekherman:Parrilo:Thomas}
\end{proof}

Therefore, we have redueced our problem decision problem to finding this positive semidefinite matrix. It turns out that this can be done via \emph{Semidefinite Programming} \cite{Blekherman:Parrilo:Thomas}
\begin{definition}
     An \emph{Semidefinite Problem (SDP)} in standard primal form is written as
     \begin{equation}
          \textit{minimize} \langle C, X \rangle \quad \textit{subject to} \langle A_i, X \rangle = b_i, i=1,...,k \quad X \succcurlyeq 0
     \end{equation}
\end{definition}

Returning to our problem, we require $\mathcal{Q} \succcurlyeq 0$, and the set of constraints $ \langle A_i, X \rangle = b_i$ is the same as satisfying $p(x) = \mathcal{B}_{n, d}^T \mathcal{Q} \mathcal{B}_{n, d}$, which can be done via comparing the coefficients of each element in the basis that we use. \cite{Recher:Masterthesis} 
Therefore, when we plug in the constraints into a SDP solver, if the result that we get is not no solution, then we know that the $p(x) \in \Sigma_{n, 2d}$

\begin{example}
     We can do an example here
\end{example}

Now, we further examine the constraints. Using the inner product of matrices, there is a natural representation of the constraints using the inner products $\langle \cdot,  \cdot \rangle$ between matrices. That is $\langle A, B \rangle = tr(A^T B)$ 

\begin{prop}

     We pick a basis of $\mathcal{B}_{n, d} = {b_1, ..., b_{n + d \choose d}}$ of $P_{n, d}$, and list it in a vector form $ b =\begin{bmatrix}
          b_1 &
          ... &
          b_{ n+ d \choose d}
     \end{bmatrix} ^ T$. Then by remark 2.14, we can form a basis $\mathcal{B}_{n, 2d} = \{b'_1, ..., b'_{n + 2d \choose 2d}\}$ based on the vector $b$. 
     Suppose the $p(x)$ that we are interested in is written in the form $\sum_{i = 1}^{n + 2d \choose 2d } c_i b'_i$.
     Then, we can reformulate $p(x) = b^T \mathcal{Q} b = \langle Q, b b^T \rangle$ 
\end{prop}

Because the constrants are linear with respect to $\mathcal{Q}$, we can re-write it into a system on linear equations with respect to the entries of the matrix $\mathcal{Q}$. Thus we can reformulate the constrants as
$A q = b$, where $q = [q_{1,1}, ..., q_{1, k}, q_{2, 2}, ... ,q_{k, k}]$. And we are interested in the numerical stability of $A$.

\begin{definition}
     We define the $b b^T$ in Proposition 2.19 as the \emph{Moment Matrix}.
\end{definition}



\begin{definition}
     
\end{definition}


\subsection{Polynomial Basis}
\label{Sec:polynomial Basis}

\subsection{Solving Semidefinite Program}
\label{Sec:Solving Semidefinite Program}




Toy examples maybe

\newpage
\section{Numerical Results}


\begin{prop}

\end{prop}

\begin{proof}

\end{proof}

maybe a theorem


\begin{thm}

\end{thm}

or an example...
\begin{example}

\end{example}

pictures are always a good idea...
%\begin{figure}

%\end{figure}


\newpage
\section{Maybe Some Proofs}


\newpage
\section{Resume, Outlook, or/and Open Problems}
\label{Sec:Outlook}


what did you do, what questions are still open, natural next steps etc. 

%\section*{Acknowledgements}
%We thank the anonymous referees for their helpful comments.


\newpage
\bibliographystyle{amsalpha}
\bibliography{main}

\end{document}
