\newif\ifpictures
\picturestrue

\newif\ifjournal
\journaltrue

\newif\ifArXiv
\ArXivfalse

\documentclass[12pt]{amsart}
\usepackage{amssymb,amsmath}
 \usepackage{amsopn}
 \usepackage{xspace}
  \usepackage{hyperref}
 \usepackage[dvips]{graphicx}
\usepackage[arrow,matrix,curve]{xy}
\usepackage {color, tikz}
\usepackage{wasysym}


\headheight=8pt
\topmargin=30pt 
\textheight=611pt     \textwidth=456pt
\oddsidemargin=6pt   \evensidemargin=6pt

\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{remark}[thm]{Remark}


\theoremstyle{definition}

\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}

\numberwithin{thm}{section}


\newcounter{FNC}[page]
\def\newfootnote#1{{\addtocounter{FNC}{2}$^\fnsymbol{FNC}$%
     \let\thefootnote\relax\footnotetext{$^\fnsymbol{FNC}$#1}}}

\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\B}{\mathbb{B}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\TP}{\mathbb{TP}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\tri}{\triangle}
\newcommand{\lf}{\left}
\newcommand{\ri}{\right}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow} 
\newcommand{\Ra}{\Rightarrow}
\newcommand{\La}{\Leftarrow}
\newcommand{\Lera}{\Leftrightarrow}
\newcommand{\ovl}{\overline}
\newcommand{\wh}{\widehat}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}

\newcommand\cA{{\ensuremath{\mathcal{A}}}\xspace}
\newcommand\cB{{\ensuremath{\mathcal{B}}}\xspace}
\newcommand\cC{{\ensuremath{\mathcal{C}}}\xspace}
\newcommand\cD{{\ensuremath{\mathcal{D}}}\xspace}
\newcommand\cE{{\ensuremath{\mathcal{E}}}\xspace}
\newcommand\cF{{\ensuremath{\mathcal{F}}}\xspace}
\newcommand\cG{{\ensuremath{\mathcal{G}}}\xspace}
\newcommand\cH{{\ensuremath{\mathcal{H}}}\xspace}
\newcommand\cI{{\ensuremath{\mathcal{I}}}\xspace}
\newcommand\cJ{{\ensuremath{\mathcal{J}}}\xspace}
\newcommand\cK{{\ensuremath{\mathcal{K}}}\xspace}
\newcommand\cL{{\ensuremath{\mathcal{L}}}\xspace}
\newcommand\cM{{\ensuremath{\mathcal{M}}}\xspace}
\newcommand\cN{{\ensuremath{\mathcal{N}}}\xspace}
\newcommand\cO{{\ensuremath{\mathcal{O}}}\xspace}
\newcommand\cP{{\ensuremath{\mathcal{P}}}\xspace}
\newcommand\cQ{{\ensuremath{\mathcal{Q}}}\xspace}
\newcommand\cR{{\ensuremath{\mathcal{R}}}\xspace}
\newcommand\cS{{\ensuremath{\mathcal{S}}}\xspace}
\newcommand\cT{{\ensuremath{\mathcal{T}}}\xspace}
\newcommand\cU{{\ensuremath{\mathcal{U}}}\xspace}
\newcommand\cV{{\ensuremath{\mathcal{V}}}\xspace}
\newcommand\cW{{\ensuremath{\mathcal{W}}}\xspace}
\newcommand\cX{{\ensuremath{\mathcal{X}}}\xspace}
\newcommand\cY{{\ensuremath{\mathcal{Y}}}\xspace}
\newcommand\cZ{{\ensuremath{\mathcal{Z}}}\xspace}

\newcommand{\eps}{\varepsilon}
\newcommand{\vphi}{\varphi}
\newcommand{\alp}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\kro}{\delta}
\newcommand{\sig}{\sigma}
\newcommand{\Sig}{\Sigma}
\newcommand{\lap}{\Delta}
\newcommand{\Gam}{\Gamma}


\definecolor{DarkGreen}{rgb}{0,0.65,0}
\newcommand{\yixuan}[1]{{\color{orange} \sf $\clubsuit\clubsuit\clubsuit$ Yixuan: [#1]}}
\newcommand{\mareike}[1]{{\color{cyan} \sf $\clubsuit\clubsuit\clubsuit$ Mareike: [#1]}}
\newcommand{\assum}{{\color{red} \sf $(\clubsuit)$}}
\newcommand{\durch}[1]{\textcolor{red}{\sout{#1}}}


\newcommand{\struc}[1]{{\color{blue} #1}}
\newcommand{\alert}[1]{{\color{red} #1}}


\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\tconv}{tconv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\trop}{trop}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\New}{New}
\DeclareMathOperator{\tdeg}{tdeg}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\re}{re}
\DeclareMathOperator{\im}{im} 
\DeclareMathOperator{\odd}{odd} 
\DeclareMathOperator{\even}{even} 
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Circ}{Circ}

\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\RE}{Re}
\DeclareMathOperator{\IM}{Im}
\DeclareMathOperator{\w}{\wedge}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\Val}{Val}
\DeclareMathOperator{\coA}{\text{co}\cA}
\DeclareMathOperator{\coL}{\text{co}\cL}
\DeclareMathOperator{\eq}{eq}
\DeclareMathOperator{\app}{app}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ini}{in}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\SOS}{SOS}
\DeclareMathOperator{\prep}{Prep}

\DeclareMathOperator{\verti}{Vert}
\DeclareMathOperator{\coeffs}{coeffs}


\def\endexa{\hfill$\hexagon$}

\title{Honors Project}

\author{Yixuan Zhou} %\author{}

\address{8514 Villa La Jolla Drive \# 114, La Jolla, CA, 92037\medskip}
\email{yiz044@ucsd.edu}

\address{9500 Gilmam Drive \# 0112, La Jolla, CA, 92093 \medskip}
\email{mdressler@ucsd.edu}



%\subjclass[2010]{Primary: 14P10, 90C25; Secondary: 12D05, 52B20}
\keywords{some keywords go here}


\begin{document}
 

\begin{abstract}
Short description
\end{abstract}

\maketitle


\section{Introduction} 

\emph{Polynomials} are very powerful in solving problems in applied math and engineering. 
And deciding the nonnegativity of a given polynomial become very important in many settings (polynomial optimizations and statistical regressions). 
Therefore, this decision problem become a central topic in the study of real algebraic geometry. 
In computational complexity terminology, it has been known that the deciding whether an arbitrary multivariate polynomial is a \emph{nonnegative polynomial} \ref{def:NGP} is a NP-hard problem (computationally infeasible) when the 
degree of the polynomial is greater than two. 

However, if we redirect our focus to determine whether a given polynomial can be expressed in terms of \emph{sum of squares (SOS)} \ref{def:SOS},
the problem is reduced to solving a \emph{semidefinite programming (SDP)} \ref{def:SDP} problem which 
has known algorithms that can solve the problem in polynomial times (efficiently) with respect to the input size. 
Though Hilbert has showed that the set of all \emph{SOS} coincide with the set of all \emph{nonnegative polynomials} only if the polynomials are univariate, quadratic, or bivariate with degree 4,
the \emph{SOS} is still the mostly well-known and well-used certificate in this decision problem due to its simplicity and the existence of efficient solvers. \cite{Blekherman:Parrilo:Thomas}

When it comes to solving the \emph{SDP}, the solver is actually dealing with a set of linear constraints, which can be compactly written in the form $Ax = b$, obtained by the process of \emph{comparison of coefficients (COC)}.
This process can be understood in terms of vectors after realizing that the set of n-variate real polynomials with degree less than or equals to $d$, denoted by $\mathbb{R}[\mathbf{x}]_{n, d}$ form a vector space.
Then any polynomial $p(\mathbf{x}) \in \mathbb{R}[\mathbf{x}]_{n, d}$ can be uniquely identified as $p(\mathbf{x}) = \sum_{j = 0}^k c_j b_j$ given a basis $\mathcal{B} = \{b_0, ..., b_k\}$ of $\mathbb{R}[\mathbf{x}]_{n, d}$.
Therefore, the process of \emph{COC} is just formulating the equality constraints by identifying the constants $c_0, ..., c_k$. \cite{Recher:Masterthesis}

Because the linear system that formed by \emph{COC} depends on the basis $\mathcal{B}$,
the stability of the linear system, measured by the \emph{condition number} \ref{def:COND}, is drastically fluctuated depends on the $\mathcal{B}$ that we chose. 
In this paper, I am going to run experiments, with the computer's aid, of different choices of the basis $\mathcal{B}$ to determine, 
in different scenarios, what is the best choice of basis to carry out the process of \emph{COC} so that the resulted linear system is most stabled. 

The paper is organized in the following: In section 2, I will introduce all the preliminary materials, 
with section 2.1 focus to introduce all the tools that will be needed, 
section 2.2 emphasized on introducing the detailed procedure of the process of \emph{COC} and the relations with the basis of the polynomial space,
and section 2.3 listing some polynomial basis that will be considered in this paper and their properties. 
In section 3, the main numerical results of the \emph{condition number} will be presented. 
And in section 4, I will present some discussion about the result that is obtained in section 3 and some thoughts on what are the further efforts that can be made to this problem.

\newpage

\centerline{\textbf{Acknowledgements}}

I would like to express my deepest gratefulness to my program advisor Mareike Dressler for offering this opportunity to do this honor project. 
During the past three quarters, she not only has been providing insightful ideas about the direction of this project should be going, 
but also has been guiding me through the process of doing research. The project could not have done without her support and help. Thank you.

\newpage

\section{Preliminaries}
\label{Sec:Preliminaries}

Here background definitions etc will be put. 

\subsection{Linear Algebra And Semidefinite Programming}
\label{Sec:Linear Algebra}

\subsection{Real Coefficient Polynomials}
\label{Sec:Real Coefficient Polynomials}

Because we are going to use a lot of tools from lienar algebra, we first introduce some key definiations that we will use in this thesis.

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times n}$, we say it is \emph{symmetric} if $A^T = A$.
\end{definition}


\begin{thm} Spectrum Theorem

     Given a symmetric matrix$A \in \mathbb{R}^{n \times n}$, it can be diagnoalized as \begin{equation} A = P^{-1}DP \end{equation} where $D$ is the diagnoal matrix with all real values, and $P$ is an orthonomal matrix.
     
     In other word, $A$ has all real eigenvalues, and their corresponding eigenvectors form an orthonomal basis of $\mathbb{R}^n$. \cite{golub1996matrix}
     
\end{thm} 


\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times n}$, it is \emph{positive semidifinate} if $A$ is symmetric and \begin{equation}
          x^T A x \geq 0 \quad \forall x \in \mathbb{R}^n
     \end{equation}
     And we denote it as $A \succcurlyeq 0$.
\end{definition}

\begin{prop}
     A matrix is \emph{positive semidifinate} if and only if all its eigenvalues are greater than or equals to 0
\end{prop}

\begin{proof}
     If a matrix $A$ has eigenvalue $\lambda < 0$, then if $x$ is the correspdoning eigenvector, we have $x^T A x = \lambda x^T x < 0$.
     On the other hand, if $A$ has all positive eignevalues, because $A$ being a symmetric matrix, its eigenvalue form a basis. Thus for any $x \in \mathbb{R}^n$, we have
     $x = \sum_{i = 1} ^ n c_i v_i $ where $v_i$ are the eigenvectors of $A$, that are also orthnormal to each other.
     Hence, $x^T A x$ = $\sum_{i = 1} ^ n \lambda_i$. Since all $\lambda_i \geq 0$, we have that $x ^ T A x \geq 0$.

\end{proof}

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times m}$, the \emph{pesudo-inverse},
     which is also knows as the \emph{Moore-Penrose} inverse of $A$, is the matrix
     $A^+$ satisfying:
     \begin{itemize}
          \item $A A^+ A = A$
          \item $A^+ A A^+ = A^+$
          \item $(A A^+)^T = A A^+$
          \item $(A^+ A)^T = A A^+$
        \end{itemize}
     
\end{definition}
\smallskip
     Every matrix has its pesudo-inverse, and when $A \in \mathbb{R}^{n \times m}$ is \emph{full rank}, 
     that is $rank(A) = min\{n, m\}$, $A$ can be expressed in simple algebric form.
     
     In particular, when $A$ has linearly independent columns, $A^+$ can be computed as
     \begin{equation}
          A^+ = (A^T A)^{-1} A^T
     \end{equation}
     In this case, the pesudo-inverse is called the \emph{left inverse} since $A^+ A = I$.

     \smallskip
     And when $A$ has linearly independent rows, $A^+$ can be be computed as
     \begin{equation}
          A^+ = A^T (A A^T)^{-1}
     \end{equation}
     In this case, the pesudo-inverse is called the \emph{right inverse} since $A A^+ = I$. 


\begin{definition}
     \label{def:COND}
     Given a matrix $A \in \mathbb{R}^{n \times m}$, the condition number of $A$, $\kappa(A)$ is defined as
     \begin{equation}
          \kappa(A) = ||A|| \cdot ||A^+||
     \end{equation}
     for any norm imposed on $A$, for instance \emph{Frobenius norm}.
\end{definition}

\begin{remark}
     The condition number measure how stable the system is. It can be alternatively defined as
     \begin{equation}
          \kappa(A) = \frac{\sigma_{max} (A)}{\sigma_{min} (A)}
     \end{equation}
     where the $\sigma$ denotes the singular values of $A$.

     Thus, it can be understand as how stale our system is. 
     Intuiatively, when the condition number is large, some error in the input along the max direction of the singular value,
     our result would largly flacuate because the error, maginified by the singular value, will dominate the input that is along
     the direction of the minimum singular value. 
     Therefore, the smaller the condition number is, the more stable our system is under flcuations caused by noises.
     The rigirous explaination of the condition numebr can be found in \cite{Cheney:Kincaid}
\end{remark}

\smallskip

Now with the tools from linear algebra, we are ready to proceed to the realm of real coefficients polynomials.

\begin{definition}
     Let $\mathbb{R}[x]_{n,d}$ denotes the set of real coefficient 
     polynmials with $n$ variables and at most $d$ degree.
\end{definition}

\begin{definition}
     \label{def:NGP}
     Let $P_{n, 2d}$ denotes the set of nonegative polynomials with 
     $n$ variables and at most $2d$ degree, that is 
     \begin{equation}
          P_{n, 2d} = \{ p \in \mathbb{R}[x]_{n, 2d}: p(x) \geq 0, \forall x \in \mathbb{R}^d \}
     \end{equation}
\end{definition}

\begin{remark}
     When trying to determine the nonnegativity of a polynomial, there is no reason to consider the set $P_{n, d}$ when $d$ is odd, since if the 
     degree of a polynomial is odd, then it will always be nonegative at some point.
\end{remark}

\begin{definition}
\label{def:SOS}
     Let $\Sigma_{n,2d}$ denotes the set of polynomials with $n$ variables and at most
     $d$ degree that are \emph{Sum of Suqares}, that is
     \begin{equation}
          \Sigma_{n, 2d} = \{ p \in \mathbb{R}[x]_{n, 2d}: \exists \text{ } q_1(x), ..., q_k(x) \in \mathbb{R}[x]_{n,d} \text{ } s.t. \text{ }  p(x) = \sum_{i=1}^k q_i^2(x)\}
     \end{equation}     
\end{definition}

\begin{remark}
     It is easy to check that $P_{n, 2d}$ form an vector space. There are a lot of choices of basis, and the most canonical one is the monomial basis.
\end{remark}

\begin{example}
     $P_(2, 2)$ is a vector space, then $\mathcal{B}_{2, 2} = {x^2, xy, y^2, x, y, 1}$ is a basis of the vector space.
\end{example}

\begin{remark}
     Given $\mathcal{B}_{n, d}$ be a basis of $P_{n, d}$. If we list the elements of $\mathcal{B}_{n, d}$ in a column vector, write as $b$, then $b b^T$ form a matrix whose upper triangle entries can be collected to form a basis of $P_{n, 2d}$.
\end{remark}

\begin{example}
     Let $\mathcal{B}_{2, 1} = {x, y, 1}$, be a basis of $P_{2, 1}$, then 
     \begin{equation}
          \begin{bmatrix}
               x \\
               y \\
               1
          \end{bmatrix}
          \begin{bmatrix}
               x & y & 1
          \end{bmatrix}
          = \begin{bmatrix}
               x^2 & xy & x \\
               xy & y^2 & y \\
               x & y & 1 \\
          \end{bmatrix}
     \end{equation}
     Then the upper triangle of the result form a basis of $P_{2, 2}$.
\end{example}

\smallskip
Then the question that we are interested in is given any polynomial $p(x) \in P_{n,2d}$, decide whether it is in $\Sigma_{n, 2d}$. 
To help solving this decision problem, the following proposition will become very helpful.

\begin{prop}
     Given $p(x) \in P_{n, 2d}$, if $p(x) \in \Sigma_{n, 2d}$, then for any basis $\mathcal{B}_{n, d}$ of $P_n{n, d}$, there exists a matrix $ \mathcal{Q} \curlyeqsucc 0$ such that
     \begin{equation}
          \mathcal{B}_{n, d} ^ T \mathcal{Q} \mathcal{B}_{n, d} = p(x)
     \end{equation}
\end{prop}

\begin{proof}
     For any $p(x) \in P_{n, 2d}$, if $p(x) \in \Sigma_{n, 2d}$, then we can write 
     \begin{equation}
          p(x) = \sum_{i = 1} ^ k q^2(x) = \begin{bmatrix} q_1(x),... ,q_k(x)] \end{bmatrix}  \begin{bmatrix} q_1 \\ \vdots \\ q_{k} \end{bmatrix}
     \end{equation}
     Notice that $q_j(x) \in P_{n, d}.$ 
     
     Now given $\mathcal{B}_{n, d} = \{b_1, ..., b_{n + d \choose d}\}$ be a basis of $P_{n, d},$ 
     we have 
     \begin{equation} q_j(x) = \sum_{i = 1}^{n + d \choose d} c_j b_j = \begin{bmatrix} c_1,..., c_{n + d \choose d}] \end{bmatrix} \begin{bmatrix} b_1 \\ \vdots \\ b_{n + d \choose d} \end{bmatrix} \end{equation}

     By subsituting the section equation into the first, we have
     \begin{equation}
          p(x) = 
               \begin{bmatrix} b_1 & ...& b_{n + d \choose d}
               \end{bmatrix} 
               \begin{bmatrix}
               c_{1,1} & ... & c_{1,k} \\
               \vdots\\
               c_{{n + d \choose d},1} & ... & c_{{n + d \choose d},k}
               \end{bmatrix}
               \begin{bmatrix}
                    c_{1,1} & ... & c_{1,{n + d \choose d}} \\
                    \vdots\\
                    c_{k,1} & ... & c_{k, {n + d \choose d}}
               \end{bmatrix}
               \begin{bmatrix} b_1 \\ \vdots \\ b_{n + d \choose d}
               \end{bmatrix} 
     \end{equation}

     Now the matrices in the middle is $C^T C = \mathcal{Q}$ a positive semidefinite matrix, which proofs the foward direction of this theorem.

     On the other hand, if we know $p(x) = \mathcal{B}_{n, d}^T \mathcal{Q} \mathcal{B}_{n, d}$, where $\mathcal{Q}$ is a positive semidefinite matrix, we can just apply the Cholesky decomposition to get $\mathcal{Q} = L^T L$, and recover the SOS form of $p(x)$. \cite{Blekherman:Parrilo:Thomas}
\end{proof}

\smallskip
Therefore, we have redueced our problem decision problem to finding this positive semidefinite matrix. It turns out that this can be done via \emph{Semidefinite Programming} \cite{Blekherman:Parrilo:Thomas}
\begin{definition}
     \label{def:SDP}
     An \emph{Semidefinite Problem (SDP)} in standard primal form is written as
     \begin{equation}
          \textit{minimize} \langle C, X \rangle \quad \textit{subject to} \langle A_i, X \rangle = b_i, i=1,...,k \quad X \succcurlyeq 0
     \end{equation}
\end{definition}
For more detail on semidefinite programming, refer to \cite{Blekherman:Parrilo:Thomas}.

Returning to our problem, we require $\mathcal{Q} \succcurlyeq 0$, and the set of constraints $ \langle A_i, X \rangle = b_i$ is the same as satisfying $p(x) = \mathcal{B}_{n, d}^T \mathcal{Q} \mathcal{B}_{n, d}$, which can be done via comparing the coefficients of each element in the basis that we use. \cite{Recher:Masterthesis} 
Therefore, when we plug in the constraints into a SDP solver, if the result that we get is not no solution, then we know that the $p(x) \in \Sigma_{n, 2d}$

\begin{example}
     We can do an example here
\end{example}

\smallskip
Now, we further examine the constraints. Using the inner product of matrices, there is a natural representation of the constraints using the inner products $\langle \cdot,  \cdot \rangle$ between matrices. That is $\langle A, B \rangle = tr(A^T B)$ 

\begin{prop}

     We pick a basis of $\mathcal{B}_{n, d} = \{b_1, ..., b_{n + d \choose d}\}$ of $P_{n, d}$, and list it in a vector form $ b =\begin{bmatrix}
          b_1 &
          ... &
          b_{ n+ d \choose d}
     \end{bmatrix} ^ T$. Then by remark 2.14, we can form a basis $\mathcal{B}_{n, 2d} = \{b'_1, ..., b'_{n + 2d \choose 2d}\}$ based on the vector $b$. 
     Suppose the $p(x)$ that we are interested in is written in the form $\sum_{i = 1}^{n + 2d \choose 2d } c_i b'_i$.
     Then, we can reformulate $p(x) = b^T \mathcal{Q} b = \langle Q, b \cdots b^T \rangle$ 
\end{prop}

\smallskip
Because the constrants are linear with respect to $\mathcal{Q}$, we can re-write it into a system on linear equations with respect to the entries of the matrix $\mathcal{Q}$. Thus we can reformulate the constrants as
$A q = b$, where $q = [q_{1,1}, ..., q_{1, k}, q_{2, 2}, ... ,q_{k, k}]$. And we are interested in the numerical stability of $A$. 

The stability of $A$ can be measured by the \emph{condition number}. Therefore, we shall seek for the base that minimize $\kappa(A)$. \cite{Recher:Masterthesis}

\begin{definition}
     We call the matrix $b \cdot b^T$ in Proposition 2.19 the \emph{Moment Matrix}.
\end{definition}

\begin{example}
     Here is another place to do an example to illustrate Moment Matrix.
\end{example}

\smallskip
Utilizing the \emph{Moment Matrix}, 
we can already measure the numerical stability for the different bases $\mathcal{B}_{n, d}$ that one can choose to carry out the comparing coefficient process to generate the constraints in the \emph{SDP}.
However, what if the polynomial $p(x)$ used a different base comparing to the \emph{moment matrix}?
\smallskip
Given the polynomial $p(x)$ in a basis $\mathcal{B'}_{n, 2d}$, one can find a change of basis matrix to convert write $p(x)$ in $\mathcal{B}_{n, 2d}$ that we will use to compare coefficients.
This change of coefficients process has its own numerical properties as well. So it would be desired if we can take that into account when analyzing the numerical properties of our constrants. 
Therefore, we shall introduce the \emph{Coefficient Moment Matrix}. In the remaining part of this section, we shall build our way to it.

\smallskip
Suppose the \emph{moment matrix} is constructed with the basis $\mathcal{B}_{n, d}$ and the polynomial $p(x)$ is written in the basis $\mathcal{B}'_{n, 2d}$. 
That is suppose $\mathcal{B}'_(n, 2d) = \{b'_0, ..., b'_k\}$ where $k = {n + 2d \choose 2d} - 1$, we have $p(x) = c_0b'_0 + ... + c_kb'_k$.
\begin{definition}
We define the \emph{coefficient extraction map} as the following map,
\begin{equation}
     \begin{split}
     \mathcal{C}: \mathbb{R}[x]_{\leq, 2d} \times \mathbb{R}[x]_{\leq, 2d} \rightarrow \mathbb{R} \\
     \mathcal{C}(p, b'_j) \mapsto c_j
     \end{split}
\end{equation}
\cite{Recher:Masterthesis}
\end{definition}

\smallskip
When fixing $s_j$, we have the \emph{coefficient extraction map} being a linear map with respect to the polynomial $p(x)$. Indeed, we have 
\begin{equation}
     \begin{split}
          \mathcal{C}(\lambda p, b'_j) = \lambda \mathcal{C}(p, b'_j) \quad \lambda \in \mathbb{R}
          \\
          \mathcal{C}(p_1 + p_2, b'_j) = \mathcal{C}(p_1, b'_j) + \mathcal{C}(p_2, b_j)
     \end{split}
\end{equation}

\begin{remark}
     When the $\mathcal{B'}_{n, 2d} = \{b'_1,... ,b'_k\}$ is a orthonomal basis, i.e. $\langle b_i, b_j \rangle \delta_{i,j}$ (the Dirac delta function), where the inner product is defined as
     \begin{equation} 
          \langle p, q \rangle = \int_a^b p(x) q(x) d\alpha(x)
     \end{equation}. 
     There is a natural concretely definiation for the \emph{coefficients extraction map}. 
     That is
     \begin{equation}
          \mathcal{C}(p(x), b'_j) = \langle p(x), b'_j \rangle
     \end{equation}
     When the basis is only orthognal, we can still define the \emph{coefficients extration map} concretely as,
     \begin{equation}
          \mathcal{C}(p(x), b'_j) = \frac{1}{||b'_j||}\langle p(x), b'_j \rangle 
     \end{equation}
     where the norm $|| \cdot ||$ is induced by the corresponding inner product.
\end{remark}

\begin{remark}
     We should actually write the \emph{coefficient extraction map} as $\mathcal{C}_{\mathcal{B'}_{n,2d}}$,
     since it depends on the base itself. However, when the base is clear, we just write it as $\mathcal{C}$.
\end{remark}

\smallskip 
We can generalize the \emph{coefficients extraction map} to take in a matrix as the first argument, and just entry-wise apply the map. With an abuse of notation, we have 
\begin{definition}
     Given a matrix of polyomials $(p_{i, j}(x))_{i, j}$ all in the bases Let the \emph{coefficient extration map} be defined as 
     \begin{equation}
          \begin{split}
               \mathcal{C}: \mathbb{R}[x]_{\leq n, 2d}^{m \times n} \times \mathbb{R}[x]_{\leq n, 2d} \rightarrow \mathbb{R}^{m \times n} & \\
               \mathcal{C}(
                    \begin{bmatrix} 
                         p_{1, 1} & ... & p_{1, n} \\
                         & \vdots \\
                         p_{m, 1} & ... & p_{m, n} \\
                    \end{bmatrix}, b'_j
               ) = \begin{bmatrix} 
                    \mathcal{C}(p_{1, 1}, b'_j) & ... &  \mathcal{C}(p_{1, n}, b'_j) \\
                    & \vdots \\
                    \mathcal{C}(p_{m, 1}, b'_j) & ... &  \mathcal{C}(p_{m, n}, b'_j) \\
                    \end{bmatrix}
          \end{split}
     \end{equation}
\end{definition}

\begin{remark}
     And immediate result from the above definition is that, given $Q \in \mathbb{R}^{m \times m}$, $M \in \mathbb{R}[x]_{n, 2d}^{m \times m}$, 
     and a basis $\mathcal{B}'_{n, 2d} = \{b'_1, ..., b'_k\}$, the matrix inner product provides the following relation,
     \begin{equation}
          \mathcal{C}(\langle Q, M \rangle, b'_j) = \langle Q, \mathcal{C}(M, b'_j) \rangle
     \end{equation}
\end{remark}

\smallskip
Notice that, let $\mathcal{B}_{n, d} = \{b_0,...,b_l\}$, where $l = {n + d \choose d} - 1$, 
let $\vec{b} = [b_1, ..., b_l]^T$, $M = \vec{b} \cdot \vec{b}^T \in \mathbb{R}^{l, l}$. 
Then given $p(x)$ in $\mathcal{B}'_{n, 2d} = \{b'_0, ..., b'_k\}$, $p = c_0,b'_0 + ... + c_k b'_k$, 
given $Q \in \mathbb{R}^{l, l}$ be the change of basis matrix from $\mathcal{B}_{n, 2d}$ to $\mathcal{B}'_{n, 2d}$, 
where $\mathcal{B}_{n, 2d}$ is generated by $\mathcal{B}_{n, d}$ using remark 2.14, we have
\begin{equation}
     c_j = \mathcal{C}(p, b'_j) = \mathcal{C}(\langle Q, M \rangle, b'_j ) = \langle Q, \mathcal{C}(M, b'_j) \rangle
\end{equation}

\begin{definition}
Define the matrix $\mathcal{A}_j = \mathcal{C}(M, b'_j)$ be the \emph{coefficient moment matrix} of $b'_j$.
\end{definition}

\begin{prop}
     $\mathcal{A}_j$ is symmetric, because $M$ is symmetric.
\end{prop}

\smallskip
Recall, the \emph{SOS} problem is to decide, given a polynomial $p(x)$, 
whether there exists a $Q \curlyeqprec 0$ such that $p = \vec{b}^TQ \vec{b}$, where $\vec{b}$ be the vector generated by $\mathcal{B}_{n, d}$.
Suppose $p(x)$ is given in $\mathcal{B'}_{n, 2d}$, we can then reformulate the constraints $\vec{b}^TQ \vec{b}$ using the \emph{coefficient moment matrix} as
\begin{equation}
     \langle Q, \mathcal{A}_j \rangle = c_j \quad \forall j = 0,...,{n + 2d \choose 2d} - 1
\end{equation} 

\smallskip
Let $\mathcal{A}_j = \begin{bmatrix}
               a_{0, 0} & a_{0, 1} & ... &a_{0, l}\\
               a_{0, 1} & a_{1, 1} & ... &a_{1, l} \\
               & \vdots \\
               a_{0, l} & a_{1, l} &... &a_{l, l}\\
     \end{bmatrix}, 
          Q = \begin{bmatrix}
               q_{0, 0} & q_{0, 1} & ... &q_{0, l}\\
               q_{0, 1} & q_{1, 1} & ... &q_{1, l} \\
               & \vdots \\
               q_{0, l} & q_{1, l} &... &q_{l, l}\\
          \end{bmatrix} $

Set $\vec{a_j} = [a_{0, 0}, 2a_{0, 1}..., 2a_{0, l}, a_{1, 1}, 2a_{1, 2}, ..., a_{l, l}]^T \in \mathbb{R}^{l + 1}$, and
$\vec{q} = [q_{0, 0}, ..., q_{l, l}]$.
We can re-write the inner product $\langle Q, \mathcal{A}_j \rangle$ using the fact that both $\mathcal{A}_j$ and $Q$ are symmetric. \begin{equation}
     \langle Q, \mathcal{A}_j \rangle = q^T \cdot a_k = a_k^T \cdot q
\end{equation}

\smallskip
Then, finally, we can re-write the constraint $p(x) = \vec{b}^T Q \vec{b}$, as the system of linear equations that 
\begin{equation}
     \mathcal{A} = \begin{bmatrix}
          a_0^T \\
          \vdots \\
          a_l^T
     \end{bmatrix} q = \begin{bmatrix}
          c_0 \\
          \vdots \\
          c_l
     \end{bmatrix}
\end{equation}
Thus, the numerical property of the \emph{SDP} problem is completely captured by the \emph{condition number} of $A$. 
Therefore, we will write python code to examine different combinations of bases under different degrees and number of variates in the Preliminaries sections. 
We will list the polynomial bases that we are interested in the following sections, and will briefly touch upon \emph{Semidifinite Programing} before we present our results.

\subsection{Polynomial Basis}
\label{Sec:polynomial Basis}

\subsection{Solving Semidefinite Program}
\label{Sec:Solving Semidefinite Program}




Toy examples maybe

\newpage
\section{Numerical Results}


\begin{prop}

\end{prop}

\begin{proof}

\end{proof}

maybe a theorem


\begin{thm}

\end{thm}

or an example...
\begin{example}

\end{example}

pictures are always a good idea...
%\begin{figure}

%\end{figure}


\newpage
\section{Maybe Some Proofs}


\newpage
\section{Resume, Outlook, or/and Open Problems}
\label{Sec:Outlook}


what did you do, what questions are still open, natural next steps etc. 

%\section*{Acknowledgements}
%We thank the anonymous referees for their helpful comments.


\newpage
\bibliographystyle{amsalpha}
\bibliography{main}

\end{document}
