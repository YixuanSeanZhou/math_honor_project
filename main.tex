\newif\ifpictures
\picturestrue

\newif\ifjournal
\journaltrue

\newif\ifArXiv
\ArXivfalse

\documentclass[12pt]{amsart}
\usepackage{amssymb,amsmath}
 \usepackage{amsopn}
 \usepackage{xspace}
  \usepackage{hyperref}
 \usepackage[dvips]{graphicx}
\usepackage[arrow,matrix,curve]{xy}
\usepackage {color, tikz}
\usepackage{wasysym}
\usepackage[ruled,vlined]{algorithm2e}


\headheight=8pt
\topmargin=30pt 
\textheight=611pt     \textwidth=456pt
\oddsidemargin=6pt   \evensidemargin=6pt

\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{remark}[thm]{Remark}


\theoremstyle{definition}

\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}

\numberwithin{thm}{section}


\newcounter{FNC}[page]
\def\newfootnote#1{{\addtocounter{FNC}{2}$^\fnsymbol{FNC}$%
     \let\thefootnote\relax\footnotetext{$^\fnsymbol{FNC}$#1}}}


\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\B}{\mathbb{B}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\TP}{\mathbb{TP}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\tri}{\triangle}
\newcommand{\lf}{\left}
\newcommand{\ri}{\right}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow} 
\newcommand{\Ra}{\Rightarrow}
\newcommand{\La}{\Leftarrow}
\newcommand{\Lera}{\Leftrightarrow}
\newcommand{\ovl}{\overline}
\newcommand{\wh}{\widehat}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}

\newcommand\cA{{\ensuremath{\mathcal{A}}}\xspace}
\newcommand\cB{{\ensuremath{\mathcal{B}}}\xspace}
\newcommand\cC{{\ensuremath{\mathcal{C}}}\xspace}
\newcommand\cD{{\ensuremath{\mathcal{D}}}\xspace}
\newcommand\cE{{\ensuremath{\mathcal{E}}}\xspace}
\newcommand\cF{{\ensuremath{\mathcal{F}}}\xspace}
\newcommand\cG{{\ensuremath{\mathcal{G}}}\xspace}
\newcommand\cH{{\ensuremath{\mathcal{H}}}\xspace}
\newcommand\cI{{\ensuremath{\mathcal{I}}}\xspace}
\newcommand\cJ{{\ensuremath{\mathcal{J}}}\xspace}
\newcommand\cK{{\ensuremath{\mathcal{K}}}\xspace}
\newcommand\cL{{\ensuremath{\mathcal{L}}}\xspace}
\newcommand\cM{{\ensuremath{\mathcal{M}}}\xspace}
\newcommand\cN{{\ensuremath{\mathcal{N}}}\xspace}
\newcommand\cO{{\ensuremath{\mathcal{O}}}\xspace}
\newcommand\cP{{\ensuremath{\mathcal{P}}}\xspace}
\newcommand\cQ{{\ensuremath{\mathcal{Q}}}\xspace}
\newcommand\cR{{\ensuremath{\mathcal{R}}}\xspace}
\newcommand\cS{{\ensuremath{\mathcal{S}}}\xspace}
\newcommand\cT{{\ensuremath{\mathcal{T}}}\xspace}
\newcommand\cU{{\ensuremath{\mathcal{U}}}\xspace}
\newcommand\cV{{\ensuremath{\mathcal{V}}}\xspace}
\newcommand\cW{{\ensuremath{\mathcal{W}}}\xspace}
\newcommand\cX{{\ensuremath{\mathcal{X}}}\xspace}
\newcommand\cY{{\ensuremath{\mathcal{Y}}}\xspace}
\newcommand\cZ{{\ensuremath{\mathcal{Z}}}\xspace}

\newcommand{\eps}{\varepsilon}
\newcommand{\vphi}{\varphi}
\newcommand{\alp}{\alpha}
\newcommand{\lam}{\lambda}
\newcommand{\kro}{\delta}
\newcommand{\sig}{\sigma}
\newcommand{\Sig}{\Sigma}
\newcommand{\lap}{\Delta}
\newcommand{\Gam}{\Gamma}


\definecolor{DarkGreen}{rgb}{0,0.65,0}
\newcommand{\yixuan}[1]{{\color{orange} \sf $\clubsuit\clubsuit\clubsuit$ Yixuan: [#1]}}
\newcommand{\mareike}[1]{{\color{cyan} \sf $\clubsuit\clubsuit\clubsuit$ Mareike: [#1]}}
\newcommand{\assum}{{\color{red} \sf $(\clubsuit)$}}
\newcommand{\durch}[1]{\textcolor{red}{\sout{#1}}}


\newcommand{\struc}[1]{{\color{blue} #1}}
\newcommand{\alert}[1]{{\color{red} #1}}


\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\tconv}{tconv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\trop}{trop}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\New}{New}
\DeclareMathOperator{\tdeg}{tdeg}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\re}{re}
\DeclareMathOperator{\im}{im} 
\DeclareMathOperator{\odd}{odd} 
\DeclareMathOperator{\even}{even} 
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Circ}{Circ}

\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\RE}{Re}
\DeclareMathOperator{\IM}{Im}
\DeclareMathOperator{\w}{\wedge}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\Val}{Val}
\DeclareMathOperator{\coA}{\text{co}\cA}
\DeclareMathOperator{\coL}{\text{co}\cL}
\DeclareMathOperator{\eq}{eq}
\DeclareMathOperator{\app}{app}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ini}{in}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\SOS}{SOS}
\DeclareMathOperator{\prep}{Prep}

\DeclareMathOperator{\verti}{Vert}
\DeclareMathOperator{\coeffs}{coeffs}


\def\endexa{\hfill$\hexagon$}

\title{Honors Project}

\author{Yixuan Zhou} %\author{}

\address{8514 Villa La Jolla Drive \# 114, La Jolla, CA, 92037\medskip}
\email{yiz044@ucsd.edu}

% \address{9500 Gilmam Drive \# 0112, La Jolla, CA, 92093 \medskip}
% \email{mdressler@ucsd.edu}



%\subjclass[2010]{Primary: 14P10, 90C25; Secondary: 12D05, 52B20}
\keywords{some keywords go here}


\begin{document}
 

\begin{abstract}
Short description
\end{abstract}

\maketitle


\section{Introduction} 

\emph{Polynomials optimizations} are very powerful in terms of expressing and solving problems in applied math and engineering. 
For instance, Ahmadi and Majumdar suggested that when applied in the field of artificial intelligence, \emph{Polynomials optimizations} can be used in solving \emph{real-time decision problems} \cite{ahmadi2015applications}.
Josz suggested that \emph{Polynomials optimizations} can also be used in solving the \emph{optimal power flow problem} in the field of electric power systems design \cite{josz:tel-01478431}. 
In such problems, one usually wants to minimize an objective function subjects to some constraints, that is,
\begin{equation*}
    \min \ f(x) \quad \textit{s.t. } g_i(x) \geq 0.
\end{equation*}

Other optimization techniques, for example the gradient decent algorithm, seeks to exploit the properties of the objective functions to find a local minimizer and hopes (with justifications in special cases) that the function value at the minimizer is close the global minimum. 
On the other hand, \emph{Polynomials optimizations} attempt to solve this problem by directly finding the global minimizer of the objective function. When we assume that we have an unconsecrated optimization problem, 
the general form of the \emph{Polynomials optimizations} can be expressed as
\begin{equation*}
     \max \ r \quad \textit{s.t. } f(x) - r \geq 0.
\end{equation*}

To solve this problem, we would need to decide whether a given polynomial, $f(x) - r$ is a \emph{nonnegative polynomial} (see Definition \ref{def:NGP}) or not.
It turns out that this problem has been well studied in real algebraic geometry dates back to the beginning of 20th century. 
However, it turns out that deciding whether an arbitrary multivariate polynomial is nonnegative is,
in the language of computational complexity, \emph{NP-Hard} (computationally infeasible). 

Therefore, instead of directly decide whether a polynomial is nonnegative, people start to seek certificates (sufficient conditions) for a polynomial being nonnegative.
One of the most famous and canonical certificates is whether a polynomial can be expressed as a \emph{sum of squares (SOS)} (see Definition \ref{def:SOS}),
This certificate problem is given by a \emph{semidefinite program (SDP)} (see Definition \ref{def:SDP}) and
has known algorithms that can compute the result in polynomial times (efficiently) with respect to the input size \cite{Blekherman:Parrilo:Thomas}.

% Though Hilbert has showed that the set of all \emph{SOS} coincide with the set of all \emph{nonnegative polynomials} only if the polynomials are univariate, quadratic, or bivariate with degree 4,
% the \emph{SOS} is still the mostly well-known and well-used certificate in this decision problem due to its simplicity and the existence of efficient solvers. \cite{Blekherman:Parrilo:Thomas}

When solving the \emph{SDP}, the solver is actually dealing with a set of linear constraints, which can be compactly written in the form $Ax = b$, obtained by the process of \emph{comparison of coefficients (COC)}.
Though this process is formally introduced in Section 2.3, the intuition behind it is quite simple. 
It can be understood in terms of vectors after realizing that the set of n-variate real polynomials with degree less than or equals to $d$, denoted by $\mathbb{R}[\mathbf{x}]_{n, d}$ forms a vector space.
Then any polynomial $p(\mathbf{x}) \in \mathbb{R}[\mathbf{x}]_{n, d}$ can be uniquely identified as $p(\mathbf{x}) = \sum_{j = 0}^k c_j b_j$ given a basis $\mathcal{B} = \{b_0, ..., b_k\}$ of $\mathbb{R}[\mathbf{x}]_{n, d}$.
Therefore, the process of \emph{COC} is just formulating the equality constraints by identifying the constants $c_0, ..., c_k$.
For more about vector space and basis, one could refer to the book Linear Algebra and Its Applications by Lay, Lay and McDonald\cite{strang2006linear}.

Because the linear system that formed by \emph{COC} depends on the basis $\mathcal{B}$,
the stability of the linear system, measured by the \emph{condition number} (see Definition \ref{def:COND}), is drastically fluctuated depends on the $\mathcal{B}$ that we chose. 
The stability of the system is very important in practice because this measure how robust the method is under noises in the data.
Yet, it is unclear what basis generates the best result.
In this paper, we perform numerical experiments, with the computer's aid, of different choices of the basis $\mathcal{B}$ to determine, 
in different scenarios, what is the best choice of basis to carry out the process of \emph{COC} so that the resulted linear system is most stabled. 

The paper is organized as follows: In Section 2, we introduce the preliminary material, 
including the tools that we need throughout this paper, and the algorithm that will be employed to carry out the \emph{COC}.
A brief survey of polynomial bases that are considered in this paper is also included in this section. 
In Section 3, the main numerical results of the \emph{condition number} is presented. 
And in Section 4, we discuses the result that is obtained in Section 3 and some thoughts on what are the further efforts that can be made to this problem.



\newpage

\centerline{\textbf{Acknowledgements}}

I would like to express my deepest gratefulness to my program advisor Mareike Dressler for offering this opportunity to do this honor project. 
During the past three quarters, she not only has been providing insightful ideas about the direction of this project should be going, 
but also has been guiding me through the process of doing research. The project could not have done without her support and help. Thank you.

\newpage

\section{Preliminaries}
\label{Sec:Preliminaries}

In this section, we introduce all the background material that is necessary to
understand the problem and describe the algorithm that is used to carry out the \emph{comparison of coefficients (COC)}
process. All the notations is also be introduced in this section, 
and a brief survey of the polynomial base that are considered in this paper is included in the end.

% The first subsection focus on related definitions and results from linear algebra. 
% The second subsection contains all the tools that we need from real algebraic geometry and \emph{semidefinite programming} for describing the algorithm \emph{(COC)}
% that will be used to solve the decision problem of whether a polynomial can be expressed as \emph{sum of squares}. 
% The third subsection will be describing the algorithm. And the last subsection contains a brief survey of the polynomial bases that are considered in this paper. 

\subsection{Real Polynomials}
\label{Sec:Real Polynomials}
First, we rigorously define the decision problem that is considered in this paper. 
Thus, in this subsection we define the terms that are used related to \emph{polynomials}.

Throughout this paper, we use bold letters for vectors, e.g. $\mathbf{x} = (x_1, ..., x_n) \in \mathbb{R}^n$.
The set of all $m$ by $n$ real matrices is denoated as $\mathbb{R}^{m \times n}$ 
The ring (set) of real-valued n-variate polynomials is denoted as $\mathbb{R}[\mathbf{x}]$, 
and the set of all $n$-variate real polynomials with degree less than or equals to $d$ is denoted as $\mathbb{R}[\mathbf{x}]_{n, d}$.

\begin{example}
     In mathematics, a polynomial is an expression consisting of variables and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponentiation of variables.
     Therefore, a polynomial takes the form, 
     $p(x, y, z) = x^4 + 2xyz - 6y + 7$ with $x, y, z$ being variables. This polynomial has $3$ variables and has degree $4$, thus it 
     belongs to $\mathbb{R}[\mathbf{x}]_{3, 4}$.
\end{example}

Some quick observations that we can get about the set $\mathbb{R}[\mathbf{x}]_{n, d}$ are:
\begin{prop}
     \label{prop:vs}
     $\mathbb{R}[\mathbf{x}]_{n, d}$ form a vector space.
\end{prop}
    
\begin{proof}
     Suppose $p(\mathbf{x}), q(\mathbf{x}) \in \mathbb{R}[\mathbf{x}]_{n, d}$, $c \in \mathbb{R}$, then we have,
     Then we have $cp \in \mathbb{R}[\mathbf{x}]_{n, d}$ because multiplication by a scalar does not increase the degree nor introduce new variables.
     And $p + q \in \mathbb{R}[\mathbf{x}]_{n, d}$ for the same reason. 

\end{proof}

\begin{prop}
     The dimension of the vector space $\mathbb{R}[\mathbf{x}]_{n, d}$ is ${n \choose d}$.
\end{prop}

\begin{proof}
     Any $n$ variate $d$ degree polynomial can be written in the form 
     \begin{equation*}c_0 + c_1 x_1 + c_2 x_2 + ... + c_{n+1} x_1^2 + c_{n+2} x_1 x_2 + ... + c_l x_n^d.
     \end{equation*}
     So the set $B = \{1, x_1, x_2, ..., x_1^2, x_1 x_2, ... x_n^d\}$ spans $\mathbb{R}[\mathbf{x}]_{n, d}$.
     Also, it is a linear independent set. Thus, it forms a basis of $\mathbb{R}[\mathbf{x}]_{n, d}$.
     By counting, the cardinality of $B$, $|B| = {n+d \choose d}$, which is thus the dimension of $\mathbb{R}[\mathbf{x}]_{n, d}$.
\end{proof}

\begin{definition}
     We denote a basis of $\mathbb{R}[\mathbf{x}]_{n, d}$ by $\mathcal{B}_{n, d}$.
\end{definition}

A canonical basis to this $\mathbb{R}[\mathbf{x}]_{n, d}$ is the one that is presented in the above proof, the monomial basis,
\begin{equation*}
     \mathcal{B}_{n, d} = \{1, x_1, x_2, ..., x_n, x_1 ^ 2, x_1 x_2, ..., x_n^d\}.
\end{equation*}

\begin{remark}
     \label{rem:upgrade}
     Given $\mathcal{B}_{n, d}$ be a basis of $\mathbb{R}[\mathbf{x}]_{n, d}$. 
     If we list the elements of $\mathcal{B}_{n, d}$ in a column vector, write as $b$, 
     then $b b^T$ form a matrix whose upper triangle entries can be collected to form a basis of $\mathbb{R}[\mathbf{x}]_{n, 2d}$.
\end{remark}

\begin{example}
     Let $\mathcal{B}_{2, 1} = \{1, x, y\}$, be a basis of $\mathbb{R}[\mathbf{x}]_{2, 1}$, then 
     \begin{equation*}
          \begin{bmatrix}
               1 \\
               x \\
               y
          \end{bmatrix}
          \begin{bmatrix}
               1 & x & y
          \end{bmatrix}
          = \begin{bmatrix}
               1 & x & y \\
               x & x^2 & xy \\
               y & xy & y^2 \\
          \end{bmatrix}
     \end{equation*}
     Then the upper triangle of the result form a basis of $\mathbb{R}[\mathbf{x}]_{2, 2}$.
\end{example}

\smallskip

Then, we define the terminologies related to the decision problem.

\begin{definition}
     \label{def:NGP}
     Let $P_{n, 2d}$ denote the set of nonnegative polynomials with 
     $n$ variables and degree at most $2d$, that is 
     \begin{equation*}
          P_{n, 2d} = \{ p \in \mathbb{R}[\mathbf{x}]_{n, 2d}: p(\mathbf{x}) \geq 0, \textit{ for all } \mathbf{x} \in \mathbb{R}^d \}.
     \end{equation*}
\end{definition}


The reason that we chose to write $2d$ in the definition is that \emph{nonnegative polynomials} always have even degrees.
A polynomial with odd degree would be negative when we fix all the other variables and move one variable to positive or negative infinity. 

\begin{definition}
     \label{def:SOS}
          Let $\Sigma_{n,2d}$ denote the set of polynomials with $n$ variables and degree at most
          $2d$ that are \emph{Sum of Squares}, that is
          \begin{equation*}
               \Sigma_{n, 2d} = \{ p \in \mathbb{R}[x]_{n, 2d}: \text{ exists } q_1(x), ..., q_k(x) \in \mathbb{R}[x]_{n,d} \text{ } s.t. \text{ }  p(x) = \sum_{i=1}^k q_i^2(x)\}.
          \end{equation*}     
\end{definition}

Notice that $\Sigma_{n,2d} \subseteqq P_{n, 2d}$ because sum of squares of real numbers are always nonnegative. 

Date backs to 1888, Hilbert showed that there are only three special cases where the $\Sigma_{n,2d} = P_{n, 2d}$: 
univariate polynomials, quadratic polynomials, and bivariate polynomials of degree four. Namely, when $n = 1$, or $d = 2$, or $n = 2$ and $d = 4$ \cite{Reznick96someconcrete}.
However, as discussed in the introduction, the decision problem of whether an arbitrary polynomial $p(\mathbf{x}) \in P_{n, 2d}$ is impossible to be solved in a computationally feasible way.
Whereas, we can efficiently decide whether $p(\mathbf{x}) \in \Sigma_{n, 2d}$ using \emph{semidefinite program}, which we introduce in next subsection. 
As a result, although using the latter certificate we fail to identify some nonnegative polynomials, but that is the trade off that we make in order to solve the problem in a computationally feasible way.

Just to reiterate, the \emph{decision problem} that is considered in this paper is the following: 
\begin{equation}
     \textit{Given } p(\mathbf{x}) \in \mathbb{R}[\mathbf{x}]_{n, 2d}, \textit{ decide whether } p(\mathbf{x}) \in \Sigma_{n, 2d} \label{eq:dp}.
\end{equation}




\subsection{Linear Algebra And Semidefinite Programming}
\label{Sec:Linear Algebra}

As we have seen that the $\mathbb{R}[\mathbf{x}]_{n, 2d}$ form a vector space, the tools from linear algebra plays an important role in the analysis. 
Besides, both \emph{SDP} and the measurement of stability of system require some tools from linear algebra. 
Thus, we devote this subsection introducing all the required tools. 

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times n}$, we say it is \emph{symmetric} if $A^T = A$. We denote the set of \emph{symmetric matrix} as $S^n$.  
\end{definition}

A famous result of \emph{symmetric matrix} is:
\begin{thm} [\cite{golub1996matrix}] Spectral Theorem

     Given a symmetric matrix$A \in \mathbb{R}^{n \times n}$, it can be diagonalized as \begin{equation*} A = P^{-1}DP. \end{equation*} where $D$ is a diagonal matrix with all real values, and $P$ is an orthonormal matrix.
     
     In other word, $A$ has all real eigenvalues, and their corresponding eigenvectors form an orthonormal basis of $\mathbb{R}^n$. 
\end{thm} 

\smallskip

Then we introduce the key idea that related to \emph{SDP}, the \emph{positive semidefinite matrix}.

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{n \times n}$, it is \emph{positive semidefinite (psd)} if $A$ is symmetric and \begin{equation*}
          x^T A x \geq 0 \quad \forall x \in \mathbb{R}^n.
     \end{equation*}
     We denote it as $A \succcurlyeq 0$.
\end{definition}

\begin{prop}
     A matrix is psd if and only if all its eigenvalues are greater than or equals to 0.
\end{prop}

\begin{proof}
     If a matrix $A$ has eigenvalue $\lambda < 0$, then if $x$ is the corresponding eigenvector, we have $x^T A x = \lambda x^T x < 0$.
     On the other hand, if $A$ has all positive eigenvalues, because $A$ being a symmetric matrix, 
     its eigenvectors form a basis. Thus, for any $x \in \mathbb{R}^n$, we have
     $x = \sum_{i = 1} ^ n c_i v_i $ where $v_i$ are the eigenvectors of $A$, that are also orthonormal to each other.
     Hence, $x^T A x$ = $\sum_{i = 1} ^ n \lambda_i$. Since all $\lambda_i \geq 0$, we have that $x ^ T A x \geq 0$.
\end{proof}

\begin{definition}
     \label{def:SDP}
     A \emph{Semidefinite Problem (SDP)} in standard primal form is written as
     \begin{equation}
          \textit{minimize } \langle C, X \rangle \quad \textit{subject to } \langle A_i, X \rangle = b_i, i=1,...,k \quad X \succcurlyeq 0 \label{eq:2.3}.
     \end{equation}
\end{definition}

\smallskip

One can compactly write the constraint $\langle A_i, X \rangle = b_i$ compactly in a matrix form, we can collect all the constraints and write it is $ \langle A, X \rangle = \mathbf{b}$ 
The stability of this linear system is then of the interest of this paper. 
The \emph{condition number} of the matrix $A$ is used to measure the stability of the above linear system. 
The \emph{condition number} can be nicely calculated using the inverse (when the matrix is square) and the \emph{pesudo-inverse} (when the matrix is rectangle) of the matrix $A$.

\begin{definition}
     Given a matrix $A \in \mathbb{R}^{m \times n}$, the \emph{pesudo-inverse},
     which is also knows as the \emph{Moore-Penrose} inverse of $A$, is the matrix
     $A^\dagger$ satisfying:
     \begin{itemize}
          \item $A A^\dagger A = A$
          \item $A^\dagger A A^\dagger = A^\dagger$
          \item $(A A^\dagger)^T = A A^\dagger$
          \item $(A^\dagger A)^T = A A^\dagger$
        \end{itemize}
\end{definition}

Every matrix has its pesudo-inverse, and when $A \in \mathbb{R}^{m \times n}$ is \emph{full rank}, 
that is $rank(A) = min\{n, m\}$, $A$ can be expressed in simple algebraic form.

In particular, when $A$ has linearly independent columns, $A^\dagger$ can be computed as
\begin{equation*}
     A^\dagger = (A^T A)^{-1} A^T.
\end{equation*}
In this case, the pesudo-inverse is called the \emph{left inverse} since $A^\dagger A = I$.

\smallskip
And when $A$ has linearly independent rows, $A^\dagger$ can be computed as
\begin{equation*}
     A^\dagger = A^T (A A^T)^{-1}.
\end{equation*}
In this case, the pesudo-inverse is called the \emph{right inverse} since $A A^\dagger = I$. 

\begin{definition}
     \label{def:COND}
     Given a matrix $A \in \mathbb{R}^{m \times n}$, the condition number of $A$, $\kappa(A)$ is defined as
     \begin{equation*}
          \kappa(A) = \begin{cases}
                ||A|| \cdot ||A^\dagger|| & \textit{ if } A \textit{ is full rank } \\
                \infty & \textit{ otherwise }
          \end{cases}
     \end{equation*}
     for any norm $|| \cdot ||$ imposed on $A$, for instance, \emph{Frobenius norm}.
\end{definition}

Here, I give a brief description of how the \emph{condition number} is related to the stability of the system by introducing another way to define it.
\begin{equation*}
     \kappa(A) = \frac{\sigma_{max} (A)}{\sigma_{min} (A)}
\end{equation*}
where the $\sigma$ denotes the singular values of $A$.

Thus, it can be understood as how stale our system is. 
Intuitively, when the condition number is large, some error in the input along the max direction of the singular value,
our result would largely fluctuate because the error, magnified by the singular value, would dominate the input that is along
the direction of the minimum singular value. 
Therefore, the smaller the condition number is, the more stable our system is under fluctuations caused by noises.
The rigorous explanation of the condition number can be found in \cite{Cheney:Kincaid}.

\subsection{Comparing Coefficient Algorithm}
With all the tools in hand, we are now ready to introduce the \emph{COC} algorithm that solves the 
decision problem described in \eqref{eq:dp}.

The algorithm is build upon the following theorem, one can find a detailed explanation of it in the third chapter of \cite{Blekherman:Parrilo:Thomas}.
\begin{thm}
     \label{thm:key}
     Given $p(x) \in P_{n, 2d}$, if $p(x) \in \Sigma_{n, 2d}$, then for any basis $\mathcal{B}_{n, d}$ of $\mathbb{R}_{n, d}$, 
     there exists a matrix such that
     \begin{equation}
          \mathcal{B}_{n, d} ^ T \mathcal{Q} \mathcal{B}_{n, d} = p(x) \textit{ and } \mathcal{Q} \succcurlyeq 0 \label{eq:2-4}.
     \end{equation}
\end{thm}

\begin{proof}
     For any $p(x) \in \mathbb{R}_{n, 2d}$, if $p(x) \in \Sigma_{n, 2d}$, then we can write 
     \begin{equation*}
          p(x) = \sum_{i = 1} ^ k q^2(x) = \begin{bmatrix} q_1(x),... ,q_k(x)] \end{bmatrix}  \begin{bmatrix} q_1 \\ \vdots \\ q_{k} \end{bmatrix}
     \end{equation*}
     Notice that $q_j(x) \in P_{n, d}.$ 
     
     Now given $\mathcal{B}_{n, d} = \{b_1, ..., b_{n + d \choose d}\}$ be a basis of $P_{n, d},$ 
     we have 
     \begin{equation*} q_j(x) = \sum_{i = 1}^{n + d \choose d} c_j b_j = \begin{bmatrix} c_1,..., c_{n + d \choose d}] \end{bmatrix} \begin{bmatrix} b_1 \\ \vdots \\ b_{n + d \choose d} \end{bmatrix} \end{equation*}

     By substituting the section equation into the first, we have
     \begin{equation*}
          p(x) = 
               \begin{bmatrix} b_1 & ...& b_{n + d \choose d}
               \end{bmatrix} 
               \begin{bmatrix}
               c_{1,1} & ... & c_{1,k} \\
               \vdots\\
               c_{{n + d \choose d},1} & ... & c_{{n + d \choose d},k}
               \end{bmatrix}
               \begin{bmatrix}
                    c_{1,1} & ... & c_{1,{n + d \choose d}} \\
                    \vdots\\
                    c_{k,1} & ... & c_{k, {n + d \choose d}}
               \end{bmatrix}
               \begin{bmatrix} b_1 \\ \vdots \\ b_{n + d \choose d}
               \end{bmatrix} 
     \end{equation*}

     Now the matrices in the middle is $C^T C = \mathcal{Q}$ a \emph{psd} matrix, which proofs the forward direction of this theorem.

     On the other hand, if we know $p(x) = \mathcal{B}_{n, d}^T \mathcal{Q} \mathcal{B}_{n, d}$ where $\mathcal{Q}$ is a \emph{psd} matrix, 
     we can just apply the Cholesky decomposition to get $\mathcal{Q} = L^T L$, and recover the SOS form of $p(x)$ as
     $\mathcal{B}_{n, d}^T L^T L \mathcal{B}_{n, d}$. 
\end{proof}

\smallskip
Therefore, we have reduced our problem decision problem to finding this \emph{psd} matrix. 
The above theorem provides a hint that the above problem can be solved via \emph{SDP}. Actually, it would be solving a sub-part of \emph{SDP}.
When examine the formulation of \emph{SDP} in \eqref{eq:2.3}, we would minimize a target function subject to a set of linear constraints and the variable being a \emph{psd} matrix. 
By examining \eqref{eq:2-4}, we can see that we have a set of constraints (later we translate this set of constraints exactly into the constraints in \emph{SDP}) 
and the requirement of $\mathcal{Q}$ being a \emph{psd}.
Therefore, we found that the existence condition that is provided in the Theorem \ref{thm:key} is a subpart of the \emph{SDP}, 
which is determining whether there is a feasible point that satisfies the constraints that are imposed. 

To address how the constraints $\mathcal{B}_{n, d} ^ T \mathcal{Q} \mathcal{B}_{n, d} = p(x)$ are translated into the constraints $\langle A, X \rangle = B$ in the \emph{SDP}, we have the following proposition.

\begin{prop}
     \label{prop:2.19}
     We pick a basis of $\mathcal{B}_{n, d} = \{b_1, ..., b_{n + d \choose d}\}$ of $\mathbb{R}_{n, d}$, and list it in a vector form $ \mathbf{b} =\begin{bmatrix}
          b_1 &
          ... &
          b_{ n+ d \choose d}
     \end{bmatrix} ^ T$. Then by Remark \ref{rem:idk}, we can form a basis $\mathcal{B}_{n, 2d} = \{b'_1, ..., b'_{n + 2d \choose 2d}\}$ from the vector $b$. 
     Suppose the $p(x)$ that we are interested in is written in the form $\sum_{i = 1}^{n + 2d \choose 2d } c_i b'_i$.
     Then, we have the reformulation of the constraints as $p(x) = \mathbf{b}^T \mathcal{Q} \mathbf{b} = \langle Q, \mathbf{bb}^T \rangle$, 
     which when written separately in different rows is exactly the formulation in Definition \ref{def:SDP}.
\end{prop}

Notice that the constraints $p(x) = \langle Q, \mathbf{bb}^T \rangle$ involved in comparing two polynomials. 
By Theorem \ref{prop:vs}, $\mathbb{R}[\mathbf{x}]_{n, 2d}$ form a vector space. 
The equality of two vectors is established by comparing the coordinates of the two vectors when under the same basis. 
Thus, when the basis of $p(x)$ is the same as the basis formed by the upper triangle of $\mathbf{bb}^T$, we can compare the 
coordinates of the vectors to establish the equality. And the coordinates for polynomials are called their coefficients. 
Thus, we name this process as \emph{Comparing of Coefficients (COC)} and the paper is designated to evaluate the stability of the constrants $p(x) = \langle Q, \mathbf{bb}^T \rangle$.

As we have mentioned, the stability is measured by the \emph{condition number} of a matrix. Thus, we would need to re-write the constraints in to the form $A x = c$. 
Therefore, the problem need to be further reformulated. And how the choices of the basis are involved in this process would also be introduced.

\begin{definition}[\cite{Recher:Masterthesis}]
     We call the matrix $bb^T$ in Proposition \ref{prop:2.19} the \emph{Moment Matrix}, we denote this matrix as $\mathcal{M}$, and it is a symmetric matrix by definition.
\end{definition}

\begin{example}
     Here is another place to do an example to illustrate Moment Matrix.
\end{example}

\smallskip
The \emph{Moment Matrix} provides the first choice of basis that is involved in the process of \emph{COC}. 
Suppose the given polynomial $p(x) = \sum_{j = 0} ^ k c_j b_j$ 
is in the same basis as the resulted basis of the \emph{Moment Matrix}, 
that is the upper triangle of $\mathbf{bb^T}$ consists $b_0, ..., b_k$. 
Because $\mathcal{Q} \succcurlyeq 0$, $\mathcal{Q}$ is symmetric, 
it is completely determined by its upper triangle.
Let $\mathbf{q} = \begin{bmatrix} q_{0,0}, ..., q_{0, m}, q_{1,1}, q_{1, 2}, ..., q_{m, m} \end{bmatrix}^T$ 
be the vector consists of all the elements of the upper triangle of $\mathcal{Q}$.
The constrants $p(x) = \langle Q, \mathbf{bb}^T \rangle$ can then be reformulated as a set of linear equations 
$A \mathbf{q} = \mathbf{c}$ where $ \mathbf{c} = \begin{bmatrix}
     c_0, ..., c_k
\end{bmatrix}^T$ 
is the vector of the coefficients of the $p(x)$, 
and $A$ is a matrix that is used to establish the equality of polynomials. 
Then, we can measure the stability of the constraints $p(x) = \langle Q, \mathbf{bb}^T \rangle$ by the \emph{condition number} of $A$. 

Since, this matrix $A$ is not the final matrix that is used in analysis, the procedure of obtaining $A$ is omitted. 

\smallskip

Now, what if the $p(x)$ is written in a basis that is different from the basis constructed by the \emph{Moment Matrix}?
One might argue that we can simply apply a change of basis matrix to convert $p(x)$ into the basis that is used in \emph{Moment Matrix}. 
However, that is no efficient and accurate way to obtain a change of basis matrix. 
The reason is that a change of basis matrix would involve writing the basis of one polynomial in terms of the basis of the other. 
This itself is a huge process of \emph{Comparing of Coefficients} and would result in an increase of perturbation to the system because the \emph{condition number} is never smaller than 1 \cite{golub1996matrix}.
Therefore, we shall introduce the \emph{Coefficient Moment Matrix}. In the remaining part of this section, we shall build our way to it.

Suppose the \emph{Moment Matrix} is constructed with the basis $\mathcal{B}_{n, d}$ and the polynomial $p(x)$ is written in the basis $\mathcal{B}'_{n, 2d}$. 
That is supposed $\mathcal{B}'_{n, 2d} = \{b'_0, ..., b'_k\}$ where $k = {n + 2d \choose 2d} - 1$, we have $p(x) = c_0b'_0 + ... + c_kb'_k$.
\begin{definition}[\cite{Recher:Masterthesis}]
     \label{def:cem}
We define the \emph{coefficient extraction map} as the following map,
\begin{equation*}
     \begin{split}
     \mathcal{C}: \mathbb{R}[x]_{\leq, 2d} \times \mathbb{R}[x]_{\leq, 2d} \rightarrow \mathbb{R} \\
     \mathcal{C}(p, b'_j) \mapsto c_j
     \end{split}
\end{equation*}
\end{definition}

\smallskip
When fixing $s_j$, we have the \emph{coefficient extraction map} being a linear map with respect to the polynomial $p(x)$. Indeed, we have 
\begin{equation*}
     \begin{split}
          \mathcal{C}(\lambda p, b'_j) = \lambda \mathcal{C}(p, b'_j) \quad \lambda \in \mathbb{R}
          \\
          \mathcal{C}(p_1 + p_2, b'_j) = \mathcal{C}(p_1, b'_j) + \mathcal{C}(p_2, b_j)
     \end{split}
\end{equation*}

\begin{remark}
     When the $\mathcal{B'}_{n, 2d} = \{b'_1,... ,b'_k\}$ is an orthonormal basis, 
     i.e. $\langle b_i, b_j \rangle \delta_{i,j}$ (the Dirac delta function), where the inner product is defined as
     \begin{equation*} 
          \langle p, q \rangle = \int_a^b p(x) q(x) d\alpha(x)
     \end{equation*}
     There is a natural concretely definition for the \emph{coefficients extraction map}. 
     That is
     \begin{equation*}
          \mathcal{C}(p(x), b'_j) = \langle p(x), b'_j \rangle
     \end{equation*}
     When the basis is only orthogonal, we can still define the \emph{coefficients extraction map} concretely as,
     \begin{equation*}
          \mathcal{C}(p(x), b'_j) = \frac{1}{||b'_j||}\langle p(x), b'_j \rangle 
     \end{equation*}
     where the norm $|| \cdot ||$ is induced by the corresponding inner product.
\end{remark}

\begin{remark}
     We should actually write the \emph{coefficient extraction map} as $\mathcal{C}_{\mathcal{B'}_{n,2d}}$,
     since it depends on the base itself. However, when the base is clear, we just write it as $\mathcal{C}$.
\end{remark}

\smallskip 
We can generalize the \emph{coefficient extraction map} to take in a matrix as the first argument, and just entry-wise apply the map. With an abuse of notation, we have 
\begin{definition}
     Given a matrix of polynomials $(p_{i, j}(x))_{i, j}$ all in the bases Let the \emph{coefficient extraction map} be defined as 
     \begin{equation*}
          \begin{split}
               & \mathcal{C}: \mathbb{R}[x]_{\leq n, 2d}^{m \times n} \times \mathbb{R}[x]_{\leq n, 2d} \rightarrow \mathbb{R}^{m \times n} \\
               & \mathcal{C}(
                    \begin{bmatrix} 
                         p_{1, 1} & ... & p_{1, n} \\
                         & \vdots \\
                         p_{m, 1} & ... & p_{m, n} \\
                    \end{bmatrix}, b'_j
               ) = \begin{bmatrix} 
                    \mathcal{C}(p_{1, 1}, b'_j) & ... &  \mathcal{C}(p_{1, n}, b'_j) \\
                    & \vdots \\
                    \mathcal{C}(p_{m, 1}, b'_j) & ... &  \mathcal{C}(p_{m, n}, b'_j) \\
                    \end{bmatrix}
          \end{split}
     \end{equation*}
\end{definition}

\begin{remark}
     An immediate result from the above definition is that, given $Q \in \mathbb{R}^{m \times m}$, $M \in \mathbb{R}[x]_{n, 2d}^{m \times m}$, 
     and a basis $\mathcal{B}'_{n, 2d} = \{b'_1, ..., b'_k\}$, the matrix inner product provides the following relation,
     \begin{equation*}
          \mathcal{C}(\langle Q, M \rangle, b'_j) = \langle Q, \mathcal{C}(M, b'_j) \rangle
     \end{equation*}
\end{remark}

\smallskip
Notice that, let $\mathcal{B}_{n, d} = \{b_0,...,b_l\}$, where $l = {n + d \choose d} - 1$, 
let $\mathbf{b} = [b_1, ..., b_l]^T$, $M = \mathbf{b} \cdot \mathbf{b}^T \in \mathbb{R}^{l, l}$. 
Then given $p(x)$ in $\mathcal{B}'_{n, 2d} = \{b'_0, ..., b'_k\}$, $p = c_0b'_0 + ... + c_k b'_k$, 
given $Q \in \mathbb{R}^{l, l}$ be the change of basis matrix from $\mathcal{B}_{n, 2d}$ to $\mathcal{B}'_{n, 2d}$, 
where $\mathcal{B}_{n, 2d}$ is generated by $\mathcal{B}_{n, d}$ using remark \ref{rem:idk}, we have
\begin{equation}
     c_j = \mathcal{C}(p, b'_j) = \mathcal{C}(\langle Q, M \rangle, b'_j ) = \langle Q, \mathcal{C}(M, b'_j) \rangle
\end{equation}

\begin{definition}
Define the matrix $\mathcal{A}_j = \mathcal{C}(M, b'_j)$ be the \emph{coefficient moment matrix} of $b'_j$.
\end{definition}

\begin{prop}
     $\mathcal{A}_j$ is symmetric, because $M$ is symmetric.
\end{prop}

\smallskip
Recall, the \emph{SOS} problem is to decide, given a polynomial $p(x)$, 
whether there exists a $Q \succcurlyeq 0$ such that $p = \mathbf{b}^TQ \mathbf{b}$, where $\mathbf{b}$ be the vector generated by $\mathcal{B}_{n, d}$.
Suppose $p(x)$ is given in $\mathcal{B'}_{n, 2d}$, we can then reformulate the constraints $\mathbf{b}^TQ \mathbf{b}$ using the \emph{coefficient moment matrix} as
\begin{equation}
     \langle Q, \mathcal{A}_j \rangle = c_j \quad \forall j = 0,...,{n + 2d \choose 2d} - 1
\end{equation} 

\smallskip
Let $\mathcal{A}_j = \begin{bmatrix}
               a_{0, 0} & a_{0, 1} & ... &a_{0, l}\\
               a_{0, 1} & a_{1, 1} & ... &a_{1, l} \\
               & \vdots \\
               a_{0, l} & a_{1, l} &... &a_{l, l}\\
     \end{bmatrix}, 
          Q = \begin{bmatrix}
               q_{0, 0} & q_{0, 1} & ... &q_{0, l}\\
               q_{0, 1} & q_{1, 1} & ... &q_{1, l} \\
               & \vdots \\
               q_{0, l} & q_{1, l} &... &q_{l, l}\\
          \end{bmatrix} $

Set $\mathbf{a_j} = [a_{0, 0}, 2a_{0, 1}..., 2a_{0, l}, a_{1, 1}, 2a_{1, 2}, ..., a_{l, l}]^T \in \mathbb{R}^{l(l+1)/2}$, and
$\mathbf{q} = [q_{0, 0}, ...,q_{1,1}, q_{1,2} ..., q_{l, l}]^T \in \mathbb{R}^{l(l+1)/2}$.
We can re-write the inner product $\langle Q, \mathcal{A}_j \rangle$ using the fact that both $\mathcal{A}_j$ and $Q$ are symmetric. \begin{equation*}
     \langle Q, \mathcal{A}_j \rangle = \mathbf{q}^T \cdot \mathbf{a_j} = \mathbf{a_j}^T \cdot \mathbf{q}
\end{equation*}

\smallskip
Then, finally, we can re-write the constraint $p(x) = \mathbf{b}^T Q \mathbf{b}$, as the system of linear equations that 
\begin{equation*}
     \mathcal{A} \mathbf{q} = \begin{bmatrix}
          a_0^T \\
          \vdots \\
          a_l^T
     \end{bmatrix} \mathbf{q} = \begin{bmatrix}
          c_0 \\
          \vdots \\
          c_l
     \end{bmatrix} = \mathbf{c}
\end{equation*}
Thus, the numerical property of the \emph{SDP} problem is completely captured by the \emph{condition number} of $A$. 
Therefore, we write code in \emph{python} to examine different combinations of bases under different degrees and number of variate in the Preliminaries sections. 
We list the polynomial bases that we are interested in the following sections, and briefly touch upon \emph{semidifinite program} before we present our results.

\subsection{Polynomial Basis}
\label{Sec:polynomial Basis}
In this section, we briefly survey the polynomial bases that is considered in this thesis,
and their associated \emph{coefficient extraction map} (defined in Definition \ref{def:cem}).
We also analyze the runtime of the \emph{coefficient extraction map}.

\subsubsection{Monomial Basis}
The most canonical basis for $\mathbb{R}[\mathbf{x}]_{n, d}$ is the monomial basis. 
\begin{equation*}
     \mathcal{B}_{n, d} = \{1, x_1, x_2, ..., x_1^2, x_1 x_2, ..., x_n^d\}
\end{equation*}

The \emph{coefficient extraction map} associated to this basis is straightforward, 
one would expand a given polynomial and group them by terms. Formally, we can capture this process with the following algorithm.

\begin{algorithm}[H]
     \SetAlgoLined
     \KwResult{Coefficient of b in p}
     \KwIn{Polynomial p, Monomial basis b, Monomial Base B}
     expand p so that it is a sum of terms in B;\\
     store coefficient of each term in a hash table H;\\
     return H[b];\\
     \caption{Coefficient Extraction Map for Monomial}
\end{algorithm}

Suppose the input polynomial $p \in \mathbb{R}[\mathbf{x}]_{n, d}$ has $k$ parentheses, has at most $m$ terms in each parenthesis,
and has $l$ characters (exclude all the operators) in it. 
The run time of the above algorithm would be $O(mk + nd + l)$.
The $O(mk)$ is the cost of operations required to expand $p$;
$O(l)$ is the cost of operations to iterate through the polynomial to group terms;
$O(nd)$ is the cost of operations to get the coefficient for each basis $b$ in $B$.

\begin{example}
     For example, the polynomial of the form $p = (3x_1 + x_2) (x_3 + x_4) + x_2$ has $k = 2$, $m = 2$, $l = 6$, and is in $\mathbb{R}[\mathbf{x}]_{2, 2}$.
\end{example}

\subsubsection{Orthogonal Polynomials}
Other than the monomial basis, the bulk of the focus of this thesis is on the orthogonal polynomial.
The reason is that due to its orthogonality, the coefficient extraction map is relatively easy to construct. 

\paragraph{Chebyshev Polynomials}
There are two kinds of polynomials are widely used in numerical analysis,
the \emph{Chebyshev Polynomial of the First Kind} and \emph{Chebyshev Polynomial of the Second Kind}.
The roots of these polynomials, called \emph{Chebyshev nodes} 
are used in polynomial interpolation because 
the resulting interpolation polynomial minimizes the effect of \emph{Runge's phenomenon}.\cite{Mathews2004Numerical}

There are many ways to generate the two sequence of polynomials in univariate settings. 
Here, we decide to include the recursive definition since this would be the most straightforward one to implement.

The univariate \emph{Chebyshev Polynomial of the First Kind} can be constructed as
\begin{align*} 
     &T_0(x) = 1 \\ 
     &T_1(x) = x \\
     &T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x)
\end{align*}
Due to its orthogonality, the associated \emph{coefficient extraction map} would be 

\begin{algorithm}[H]
     \SetAlgoLined
     \KwResult{Coefficient of b in p}
     \KwIn{Polynomial p, Chebyshev First Kind basis b, Chebyshev First Kind Base B}
     c = $\int_{-1}^1 \frac{pb}{\sqrt{1-x^2}} dx$;\\
     n = $\int_{-1}^1 \frac{bb}{\sqrt{1-x^2}} dx$;\\
     return c/n;\\
     \caption{Coefficient Extraction Map for Chebyshev First Kind}
\end{algorithm}

When handling multivariate cases, we can again use Remark \ref{rem:upgrade} to obtain bases in higher dimensions.

The \emph{coefficient extraction map} seems to be tedious to define in higher dimension cases.
However, building upon the idea of Mádi-Nagy we have the following proposition which enable us to easily 
construct \emph{coefficient extraction map} for orthogonal polynomials in higher dimensions. \cite{M-Nagy2012}

\begin{prop}
     \label{prop:COOLPROP}
     Given an orthogonal univariate polynomial base $\mathcal{B} = \{b_1, ..., b_n\}$, and assume the orthogonality is under the inner product 
     \begin{equation*}
          \langle b_i, b_j \rangle = \int_l^u b_i(x)b_j(x)w(x) dx,
     \end{equation*}
     where $w(x)$ is a weight function, 
     then the multivariate polynomial base generated using Remark \ref{rem:upgrade} is also orthogonal, and the corresponding inner product is
     \begin{equation*}
          \langle b_i, b_j \rangle = \int_l^u...\int_l^u  b_i(x)b_j(x)w(x_1)...w(x_n) dx_1 ... dx_n.
     \end{equation*}
\end{prop}

\begin{proof}
     Consider the case where we start with an orthogonal univariate polynomial base $\mathcal{B} = \{b_1(x), ..., b_n(x)\}$. 
     Then, using Remark \ref{rem:upgrade} to construct a base for bivariate polynomials, we would get 
     \begin{equation*}
          \mathcal{B}' =  \{b'_1(x, y), ..., b'_{n^2}(x, y)\} = \{b_1(x)b_1(y), b_1(x)b_2(y), ..., b_2(x)b_1(y), ..., b_n(x)b_n(y)\}.
     \end{equation*}
     Now it is immediate from Fubini's theorem that
     \begin{align*}
          \int_l^u \int_l^u  b'_i(x, y)b'_j(x, y)w(x)w(y) dx dy  & = \int_l^u \int_l^u  b_{i_1}(x) b_{j_1}(y) b_{i_2}(x)b_{j_2}(y) w(x)w(y) dx dy \\
               & = \int_l^u  b_{i_1}(x) b_{i_2}(x) w(x) dx  \int_l^u b_{j_1}(y) b_{j_2}(y) w(y) dy \\
               & = \begin{cases}
                    C & if i_1 = i_2 \textit{ and } j_1 = j_2\\
                    0 & \textit{ otherwise }
              \end{cases},
     \end{align*}
     where $c \neq 0$ is the norm of a basis $b' \in mathcal{B}'$.
     Inductively, one can generalize this to higher dimensions as well.
\end{proof}

Now using this Proposition \ref{prop:COOLPROP}, we have the \emph{coefficient extraction map} for multivariate \emph{ChebyShev Polynomials of the First Kind} be
\begin{equation*}
     \mathcal{C}(p(\mathbf{x}), b(\mathbf{x})) = \int_{-1}^1 \frac{p(x_1, ..., x_n)b(x_1, ..., x_n)}{\sqrt{1-x_1^2} \sqrt{1-x_2^2} ... \sqrt{1-x_n^2}} dx_1dx_2 ... dx_n.
\end{equation*} 

The univariate \emph{Chebyshev Polynomial of the Second Kind} can be constructed as
\begin{align*} 
     &U_0(x) = 1 \\ 
     &U_1(x) = 2x \\
     &U_{n+1}(x) = 2x U_n(x) - U_{n-1}(x).
\end{align*}

The associated \emph{coefficient extraction map} would be

\begin{algorithm}[H]
     \SetAlgoLined
     \KwResult{Coefficient of b in p}
     \KwIn{Polynomial p, Chebyshev Second Kind basis b, Chebyshev Second Kind Base B}
     c = $\int_{-1}^1 pb\sqrt{1-x^2} dx$;\\
     n = $\int_{-1}^1 bb\sqrt{1-x^2} dx$;\\
     return c/n;\\
     \caption{Coefficient Extraction Map for Chebyshev Second Kind}
\end{algorithm}

And again, by the same process described above, we can construct multivariate \emph{Chebyshev Polynomial of the Second Kind} using Remark \ref{rem:upgrade},
and we can construct the associated \emph{coefficient extraction map} using Proposition \ref{prop:COOLPROP}.




\subsection{Solving Semidefinite Program}
\label{Sec:Solving Semidefinite Program}




Toy examples maybe

\newpage
\section{Numerical Results}


\begin{prop}

\end{prop}

\begin{proof}

\end{proof}

maybe a theorem


\begin{thm}

\end{thm}

or an example...
\begin{example}

\end{example}

pictures are always a good idea...
%\begin{figure}

%\end{figure}


\newpage
\section{Maybe Some Proofs}


\newpage
\section{Resume, Outlook, or/and Open Problems}
\label{Sec:Outlook}


what did you do, what questions are still open, natural next steps etc. 

%\section*{Acknowledgements}
%We thank the anonymous referees for their helpful comments.


\newpage
\bibliographystyle{amsalpha}
\bibliography{main}

\end{document}
